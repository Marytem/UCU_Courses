{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Передбачення зарплат на IT-ринку України"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому завданні ви працюватимете з реальними даними з [зарплатного опитування DOU.ua за травень 2016р](https://dou.ua/lenta/articles/salary-report-may-june-2016/). Ви реалізуєте зважену лінійну регресію, яка передбачає зарплати Java-інженерів, та навчите свою модель за допомогою градієнтного спуску.\n",
    "\n",
    "Заповніть пропущений код в розділі «Моделювання» (позначено коментарями) та запустіть розділ «Тестування», щоб перевірити його правильність."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Підготовка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries = pd.read_csv(\"data/2016_may_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оберемо тільки Java-інженерів з-поміж усіх респондентів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java = pd.DataFrame(df_salaries[(df_salaries[\"Язык.программирования\"] == \"Java\") &\n",
    "                                   (df_salaries[\"cls\"] == \"DEV\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейменуємо деякі колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.rename(\n",
    "    columns={\n",
    "        \"exp\": \"TotalExperience\",\n",
    "        \"loc\": \"Location\"\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо рівень англійської мови числами від 1 (найнижчий) до 5 (найвищий):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java[\"EnglishLevel\"] = df_java[\"Уровень.английского\"].map({\n",
    "    \"элементарный\": 1,\n",
    "    \"ниже среднего\": 2,\n",
    "    \"средний\": 3,\n",
    "    \"выше среднего\": 4,\n",
    "    \"продвинутый\": 5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо колонку Location (найбільші IT-міста або \"other\") за допомогою one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_columns = [\n",
    "    \"LocationOther\",\n",
    "    \"LocationDnipro\",\n",
    "    \"LocationKyiv\",\n",
    "    \"LocationLviv\",\n",
    "    \"LocationOdesa\",\n",
    "    \"LocationKharkiv\"\n",
    "]\n",
    "df_java[city_columns] = pd.get_dummies(df_java[\"Location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відберемо такі ознаки:\n",
    "\n",
    "* Загальна кількість років досвіду\n",
    "* Рівень англійської мови\n",
    "* Місто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"TotalExperience\", \"EnglishLevel\"] + city_columns\n",
    "df_X = df_java[feature_columns]\n",
    "df_y = df_java[[\"salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (929, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", df_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalExperience</th>\n",
       "      <th>EnglishLevel</th>\n",
       "      <th>LocationOther</th>\n",
       "      <th>LocationDnipro</th>\n",
       "      <th>LocationKyiv</th>\n",
       "      <th>LocationLviv</th>\n",
       "      <th>LocationOdesa</th>\n",
       "      <th>LocationKharkiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TotalExperience  EnglishLevel  LocationOther  LocationDnipro  \\\n",
       "5               0.5             3              1               0   \n",
       "7               5.0             1              0               0   \n",
       "17              0.0             3              0               0   \n",
       "27              4.0             3              0               0   \n",
       "28              6.0             4              0               0   \n",
       "39              3.0             4              0               1   \n",
       "46              2.0             4              0               0   \n",
       "49              3.0             3              0               0   \n",
       "59              2.0             3              0               0   \n",
       "89              1.0             5              0               0   \n",
       "\n",
       "    LocationKyiv  LocationLviv  LocationOdesa  LocationKharkiv  \n",
       "5              0             0              0                0  \n",
       "7              1             0              0                0  \n",
       "17             1             0              0                0  \n",
       "27             1             0              0                0  \n",
       "28             1             0              0                0  \n",
       "39             0             0              0                0  \n",
       "46             0             0              0                1  \n",
       "49             1             0              0                0  \n",
       "59             0             0              0                1  \n",
       "89             1             0              0                0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    salary\n",
       "5      500\n",
       "7     1600\n",
       "17     600\n",
       "27    3400\n",
       "28    2880\n",
       "39    1425\n",
       "46    1700\n",
       "49    1800\n",
       "59    1235\n",
       "89    1200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розділимо вибірку на навчальну та тестову:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_assignment = np.random.uniform(size=len(df_X))\n",
    "\n",
    "X_train = df_X[dataset_assignment <= training_set_size].values\n",
    "y_train = df_y[dataset_assignment <= training_set_size].values.flatten()\n",
    "\n",
    "X_test = df_X[dataset_assignment > training_set_size].values\n",
    "y_test = df_y[dataset_assignment > training_set_size].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб градієнтний спуск швидше збігався, нормалізуємо навчальну вибірку так, щоб кожна ознака мала $\\mu = 0, \\sigma = 1$:\n",
    "\n",
    "$ x' = \\frac{x - \\bar{x}}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = np.average(X_train, axis=0)\n",
    "feature_sigmas = np.std(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - feature_means) / feature_sigmas\n",
    "X_test = (X_test - feature_means) / feature_sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додаємо уявну ознаку $x_0 = 1$ (intercept term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all(X_train[:, 0] == 1):\n",
    "    X_train = np.insert(X_train, 0, values=1, axis=1)\n",
    "    \n",
    "if not np.all(X_test[:, 0] == 1):\n",
    "    X_test = np.insert(X_test, 0, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:  (741, 9)\n",
      "y train:  (741,)\n",
      "\n",
      "X test:   (188, 9)\n",
      "y test:   (188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train: \", X_train.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print()\n",
    "print(\"X test:  \", X_test.shape)\n",
    "print(\"y test:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Моделювання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію гіпотези лінійної регресії в матричній формі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_linear(theta, X):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Compute the hypothesis function for linear regression.\n",
    "    return np.dot(X, theta)\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію зважування всіх навчальних прикладів $x^{(i)}$, якщо нам дана точка передбачення $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_example_weights(X, x_pred, tau):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Compute the weight for each example, given the\n",
    "    # prediction point (x_pred).\n",
    "    weights = np.array([np.exp(  (-(np.dot(X[i]-x_pred, X[i]-x_pred)))  /(2*(tau**2))) for i in range(X.shape[0])])\n",
    "    # ====================================================\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(theta, X, y, weights):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the overall loss on the training set (X),\n",
    "    # taking the weights into account.\n",
    "    return np.sum(np.matmul(np.square(predict_linear(theta, X) - y), weights)) / (2 * len(y))\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте обчислення градієнта функції втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function_gradient(theta, X, y, weights):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the gradient of the cost function on the\n",
    "    # training set (X), taking the weights into account.\n",
    "\n",
    "    return ((predict_linear(theta, X)-y)*weights).dot(X) / X.shape[0]\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте один крок градієнтного спуску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_model_weights(theta, learning_rate, cost_gradient):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Given the learning rate and the gradient of the\n",
    "    # cost function, take one gradient descent step and\n",
    "    # return the updated vector theta.\n",
    "    \n",
    "    return theta - learning_rate * cost_gradient\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Навчаємо модель:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weights, loss_fun, grad_fun, learning_rate, convergence_threshold, max_iters, verbose=False):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        loss = loss_fun(theta, X, y, weights)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: {0:3} Loss: {1}\".format(i + 1, loss))\n",
    "\n",
    "        if len(losses) > 2 and np.abs(losses[-1] - losses[-2]) <= convergence_threshold:\n",
    "            break\n",
    "        \n",
    "        grad = grad_fun(theta, X, y, weights)\n",
    "        theta = update_model_weights(theta, learning_rate, grad)\n",
    "        \n",
    "    return theta, np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Передбачення нових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_weighted_linear(X, y, x_pred, verbose=False):\n",
    "    weights = get_example_weights(X, x_pred, tau=0.1)\n",
    "    theta, losses = gradient_descent(\n",
    "        X,\n",
    "        y,\n",
    "        weights,\n",
    "        loss_fun=cost_function,\n",
    "        grad_fun=cost_function_gradient,\n",
    "        learning_rate=0.005,\n",
    "        convergence_threshold=0.0001,\n",
    "        max_iters=500,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return predict_linear(theta, x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pred = X_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   1 Loss: 2051.275762424122\n",
      "[ -3.27845509   0.29363146  -2.67014128   1.39793795 -11.14642043\n",
      "   2.85826953   1.26480915   0.76222533   1.39793795]\n",
      "Iteration:   2 Loss: 2050.4935107209703\n",
      "[ -3.27781037   0.29357369  -2.66961618   1.39766304 -11.14422845\n",
      "   2.85770744   1.26456042   0.76207543   1.39766304]\n",
      "Iteration:   3 Loss: 2049.711566651771\n",
      "[ -3.27716578   0.29351593  -2.6690912    1.39738818 -11.14203691\n",
      "   2.85714547   1.26431174   0.76192557   1.39738818]\n",
      "Iteration:   4 Loss: 2048.929930095543\n",
      "[ -3.27652132   0.29345817  -2.66856631   1.39711338 -11.13984579\n",
      "   2.8565836    1.26406311   0.76177573   1.39711338]\n",
      "Iteration:   5 Loss: 2048.148600931351\n",
      "[ -3.27587698   0.29340043  -2.66804153   1.39683864 -11.13765511\n",
      "   2.85602185   1.26381453   0.76162593   1.39683864]\n",
      "Iteration:   6 Loss: 2047.3675790383077\n",
      "[ -3.27523277   0.29334271  -2.66751685   1.39656394 -11.13546486\n",
      "   2.8554602    1.26356599   0.76147615   1.39656394]\n",
      "Iteration:   7 Loss: 2046.5868642955743\n",
      "[ -3.27458868   0.29328499  -2.66699228   1.39628931 -11.13327504\n",
      "   2.85489867   1.26331751   0.7613264    1.39628931]\n",
      "Iteration:   8 Loss: 2045.8064565823574\n",
      "[ -3.27394473   0.29322728  -2.66646781   1.39601472 -11.13108565\n",
      "   2.85433724   1.26306908   0.76117669   1.39601472]\n",
      "Iteration:   9 Loss: 2045.0263557779133\n",
      "[ -3.2733009    0.29316959  -2.66594344   1.39574019 -11.12889669\n",
      "   2.85377593   1.26282069   0.761027     1.39574019]\n",
      "Iteration:  10 Loss: 2044.2465617615442\n",
      "[ -3.27265719   0.29311191  -2.66541917   1.39546572 -11.12670816\n",
      "   2.85321473   1.26257235   0.76087734   1.39546572]\n",
      "Iteration:  11 Loss: 2043.4670744126006\n",
      "[ -3.27201361   0.29305423  -2.66489501   1.39519129 -11.12452006\n",
      "   2.85265363   1.26232406   0.76072771   1.39519129]\n",
      "Iteration:  12 Loss: 2042.6878936104804\n",
      "[ -3.27137016   0.29299657  -2.66437095   1.39491692 -11.12233239\n",
      "   2.85209265   1.26207582   0.76057811   1.39491692]\n",
      "Iteration:  13 Loss: 2041.909019234629\n",
      "[ -3.27072684   0.29293893  -2.66384699   1.39464261 -11.12014515\n",
      "   2.85153178   1.26182763   0.76042854   1.39464261]\n",
      "Iteration:  14 Loss: 2041.1304511645385\n",
      "[ -3.27008364   0.29288129  -2.66332314   1.39436835 -11.11795834\n",
      "   2.85097102   1.26157949   0.760279     1.39436835]\n",
      "Iteration:  15 Loss: 2040.3521892797496\n",
      "[ -3.26944057   0.29282366  -2.66279939   1.39409414 -11.11577196\n",
      "   2.85041036   1.2613314    0.76012949   1.39409414]\n",
      "Iteration:  16 Loss: 2039.5742334598485\n",
      "[ -3.26879762   0.29276605  -2.66227574   1.39381999 -11.11358601\n",
      "   2.84984982   1.26108335   0.75998001   1.39381999]\n",
      "Iteration:  17 Loss: 2038.796583584471\n",
      "[ -3.26815481   0.29270844  -2.6617522    1.39354589 -11.1114005\n",
      "   2.84928939   1.26083536   0.75983056   1.39354589]\n",
      "Iteration:  18 Loss: 2038.0192395332986\n",
      "[ -3.26751211   0.29265085  -2.66122876   1.39327185 -11.10921541\n",
      "   2.84872907   1.26058741   0.75968114   1.39327185]\n",
      "Iteration:  19 Loss: 2037.2422011860606\n",
      "[ -3.26686955   0.29259327  -2.66070542   1.39299786 -11.10703075\n",
      "   2.84816886   1.26033951   0.75953174   1.39299786]\n",
      "Iteration:  20 Loss: 2036.4654684225336\n",
      "[ -3.26622711   0.2925357   -2.66018218   1.39272392 -11.10484652\n",
      "   2.84760876   1.26009166   0.75938238   1.39272392]\n",
      "Iteration:  21 Loss: 2035.6890411225422\n",
      "[ -3.2655848    0.29247814  -2.65965905   1.39245004 -11.10266272\n",
      "   2.84704877   1.25984386   0.75923304   1.39245004]\n",
      "Iteration:  22 Loss: 2034.9129191659574\n",
      "[ -3.26494261   0.29242059  -2.65913602   1.39217621 -11.10047935\n",
      "   2.84648889   1.25959611   0.75908374   1.39217621]\n",
      "Iteration:  23 Loss: 2034.1371024326968\n",
      "[ -3.26430055   0.29236306  -2.6586131    1.39190243 -11.09829641\n",
      "   2.84592912   1.25934841   0.75893446   1.39190243]\n",
      "Iteration:  24 Loss: 2033.3615908027277\n",
      "[ -3.26365861   0.29230553  -2.65809027   1.39162871 -11.09611389\n",
      "   2.84536946   1.25910075   0.75878522   1.39162871]\n",
      "Iteration:  25 Loss: 2032.5863841560613\n",
      "[ -3.26301681   0.29224802  -2.65756755   1.39135504 -11.09393181\n",
      "   2.84480991   1.25885315   0.758636     1.39135504]\n",
      "Iteration:  26 Loss: 2031.811482372758\n",
      "[ -3.26237512   0.29219052  -2.65704493   1.39108143 -11.09175016\n",
      "   2.84425047   1.25860559   0.75848681   1.39108143]\n",
      "Iteration:  27 Loss: 2031.0368853329257\n",
      "[ -3.26173357   0.29213303  -2.65652242   1.39080787 -11.08956893\n",
      "   2.84369114   1.25835808   0.75833765   1.39080787]\n",
      "Iteration:  28 Loss: 2030.2625929167189\n",
      "[ -3.26109214   0.29207555  -2.656        1.39053436 -11.08738814\n",
      "   2.84313192   1.25811062   0.75818852   1.39053436]\n",
      "Iteration:  29 Loss: 2029.488605004338\n",
      "[ -3.26045084   0.29201808  -2.65547769   1.39026091 -11.08520777\n",
      "   2.84257281   1.25786321   0.75803942   1.39026091]\n",
      "Iteration:  30 Loss: 2028.714921476033\n",
      "[ -3.25980966   0.29196062  -2.65495549   1.38998751 -11.08302783\n",
      "   2.84201381   1.25761585   0.75789035   1.38998751]\n",
      "Iteration:  31 Loss: 2027.9415422120983\n",
      "[ -3.25916861   0.29190318  -2.65443338   1.38971416 -11.08084832\n",
      "   2.84145492   1.25736853   0.75774131   1.38971416]\n",
      "Iteration:  32 Loss: 2027.1684670928776\n",
      "[ -3.25852768   0.29184575  -2.65391138   1.38944087 -11.07866924\n",
      "   2.84089614   1.25712127   0.7575923    1.38944087]\n",
      "Iteration:  33 Loss: 2026.3956959987606\n",
      "[ -3.25788688   0.29178832  -2.65338948   1.38916764 -11.07649059\n",
      "   2.84033747   1.25687405   0.75744332   1.38916764]\n",
      "Iteration:  34 Loss: 2025.623228810184\n",
      "[ -3.25724621   0.29173091  -2.65286768   1.38889445 -11.07431237\n",
      "   2.83977891   1.25662688   0.75729436   1.38889445]\n",
      "Iteration:  35 Loss: 2024.851065407632\n",
      "[ -3.25660566   0.29167351  -2.65234599   1.38862132 -11.07213457\n",
      "   2.83922046   1.25637977   0.75714544   1.38862132]\n",
      "Iteration:  36 Loss: 2024.0792056716346\n",
      "[ -3.25596524   0.29161612  -2.6518244    1.38834824 -11.0699572\n",
      "   2.83866211   1.25613269   0.75699654   1.38834824]\n",
      "Iteration:  37 Loss: 2023.3076494827715\n",
      "[ -3.25532495   0.29155874  -2.65130291   1.38807522 -11.06778026\n",
      "   2.83810388   1.25588567   0.75684768   1.38807522]\n",
      "Iteration:  38 Loss: 2022.5363967216654\n",
      "[ -3.25468478   0.29150138  -2.65078152   1.38780225 -11.06560375\n",
      "   2.83754576   1.2556387    0.75669884   1.38780225]\n",
      "Iteration:  39 Loss: 2021.7654472689892\n",
      "[ -3.25404474   0.29144402  -2.65026024   1.38752934 -11.06342767\n",
      "   2.83698775   1.25539177   0.75655003   1.38752934]\n",
      "Iteration:  40 Loss: 2020.9948010054623\n",
      "[ -3.25340482   0.29138668  -2.64973906   1.38725648 -11.06125201\n",
      "   2.83642985   1.2551449    0.75640126   1.38725648]\n",
      "Iteration:  41 Loss: 2020.2244578118496\n",
      "[ -3.25276503   0.29132935  -2.64921798   1.38698367 -11.05907679\n",
      "   2.83587206   1.25489807   0.75625251   1.38698367]\n",
      "Iteration:  42 Loss: 2019.454417568963\n",
      "[ -3.25212536   0.29127203  -2.648697     1.38671091 -11.05690199\n",
      "   2.83531437   1.25465129   0.75610379   1.38671091]\n",
      "Iteration:  43 Loss: 2018.684680157663\n",
      "[ -3.25148582   0.29121472  -2.64817613   1.38643821 -11.05472762\n",
      "   2.8347568    1.25440456   0.7559551    1.38643821]\n",
      "Iteration:  44 Loss: 2017.9152454588557\n",
      "[ -3.25084641   0.29115742  -2.64765536   1.38616557 -11.05255367\n",
      "   2.83419934   1.25415788   0.75580644   1.38616557]\n",
      "Iteration:  45 Loss: 2017.1461133534935\n",
      "[ -3.25020712   0.29110013  -2.64713469   1.38589297 -11.05038016\n",
      "   2.83364198   1.25391124   0.75565781   1.38589297]\n",
      "Iteration:  46 Loss: 2016.377283722577\n",
      "[ -3.24956796   0.29104285  -2.64661412   1.38562043 -11.04820707\n",
      "   2.83308474   1.25366466   0.7555092    1.38562043]\n",
      "Iteration:  47 Loss: 2015.6087564471532\n",
      "[ -3.24892892   0.29098559  -2.64609366   1.38534795 -11.0460344\n",
      "   2.83252761   1.25341812   0.75536063   1.38534795]\n",
      "Iteration:  48 Loss: 2014.8405314083145\n",
      "[ -3.24829001   0.29092834  -2.6455733    1.38507551 -11.04386217\n",
      "   2.83197058   1.25317163   0.75521209   1.38507551]\n",
      "Iteration:  49 Loss: 2014.0726084872015\n",
      "[ -3.24765122   0.29087109  -2.64505304   1.38480313 -11.04169036\n",
      "   2.83141367   1.25292519   0.75506357   1.38480313]\n",
      "Iteration:  50 Loss: 2013.304987565002\n",
      "[ -3.24701256   0.29081386  -2.64453288   1.38453081 -11.03951898\n",
      "   2.83085686   1.2526788    0.75491509   1.38453081]\n",
      "Iteration:  51 Loss: 2012.537668522949\n",
      "[ -3.24637403   0.29075664  -2.64401282   1.38425854 -11.03734803\n",
      "   2.83030016   1.25243246   0.75476663   1.38425854]\n",
      "Iteration:  52 Loss: 2011.770651242323\n",
      "[ -3.24573562   0.29069943  -2.64349287   1.38398632 -11.0351775\n",
      "   2.82974358   1.25218616   0.7546182    1.38398632]\n",
      "Iteration:  53 Loss: 2011.0039356044508\n",
      "[ -3.24509734   0.29064224  -2.64297302   1.38371415 -11.0330074\n",
      "   2.8291871    1.25193992   0.75446981   1.38371415]\n",
      "Iteration:  54 Loss: 2010.2375214907074\n",
      "[ -3.24445918   0.29058505  -2.64245327   1.38344204 -11.03083773\n",
      "   2.82863073   1.25169372   0.75432144   1.38344204]\n",
      "Iteration:  55 Loss: 2009.471408782512\n",
      "[ -3.24382115   0.29052788  -2.64193363   1.38316998 -11.02866848\n",
      "   2.82807447   1.25144757   0.7541731    1.38316998]\n",
      "Iteration:  56 Loss: 2008.7055973613324\n",
      "[ -3.24318324   0.29047071  -2.64141408   1.38289798 -11.02649966\n",
      "   2.82751833   1.25120147   0.75402479   1.38289798]\n",
      "Iteration:  57 Loss: 2007.940087108682\n",
      "[ -3.24254546   0.29041356  -2.64089464   1.38262603 -11.02433127\n",
      "   2.82696229   1.25095542   0.75387651   1.38262603]\n",
      "Iteration:  58 Loss: 2007.174877906121\n",
      "[ -3.2419078    0.29035642  -2.6403753    1.38235413 -11.0221633\n",
      "   2.82640636   1.25070941   0.75372825   1.38235413]\n",
      "Iteration:  59 Loss: 2006.4099696352566\n",
      "[ -3.24127027   0.29029929  -2.63985606   1.38208229 -11.01999576\n",
      "   2.82585053   1.25046346   0.75358003   1.38208229]\n",
      "Iteration:  60 Loss: 2005.6453621777418\n",
      "[ -3.24063287   0.29024217  -2.63933693   1.3818105  -11.01782865\n",
      "   2.82529482   1.25021755   0.75343184   1.3818105 ]\n",
      "Iteration:  61 Loss: 2004.8810554152772\n",
      "[ -3.23999559   0.29018506  -2.63881789   1.38153876 -11.01566196\n",
      "   2.82473922   1.24997169   0.75328367   1.38153876]\n",
      "Iteration:  62 Loss: 2004.117049229608\n",
      "[ -3.23935843   0.29012797  -2.63829896   1.38126708 -11.0134957\n",
      "   2.82418373   1.24972588   0.75313554   1.38126708]\n",
      "Iteration:  63 Loss: 2003.3533435025288\n",
      "[ -3.2387214    0.29007088  -2.63778013   1.38099545 -11.01132987\n",
      "   2.82362834   1.24948012   0.75298743   1.38099545]\n",
      "Iteration:  64 Loss: 2002.5899381158783\n",
      "[ -3.2380845    0.29001381  -2.63726141   1.38072387 -11.00916446\n",
      "   2.82307307   1.24923441   0.75283935   1.38072387]\n",
      "Iteration:  65 Loss: 2001.8268329515427\n",
      "[ -3.23744772   0.28995675  -2.63674278   1.38045235 -11.00699947\n",
      "   2.8225179    1.24898874   0.75269131   1.38045235]\n",
      "Iteration:  66 Loss: 2001.0640278914534\n",
      "[ -3.23681107   0.2898997   -2.63622426   1.38018088 -11.00483491\n",
      "   2.82196285   1.24874312   0.75254329   1.38018088]\n",
      "Iteration:  67 Loss: 2000.301522817591\n",
      "[ -3.23617454   0.28984266  -2.63570584   1.37990946 -11.00267078\n",
      "   2.8214079    1.24849755   0.7523953    1.37990946]\n",
      "Iteration:  68 Loss: 1999.5393176119794\n",
      "[ -3.23553814   0.28978563  -2.63518752   1.3796381  -11.00050707\n",
      "   2.82085306   1.24825203   0.75224734   1.3796381 ]\n",
      "Iteration:  69 Loss: 1998.777412156691\n",
      "[ -3.23490186   0.28972861  -2.6346693    1.37936679 -10.99834379\n",
      "   2.82029833   1.24800656   0.75209941   1.37936679]\n",
      "Iteration:  70 Loss: 1998.015806333843\n",
      "[ -3.23426571   0.2896716   -2.63415119   1.37909553 -10.99618094\n",
      "   2.81974371   1.24776114   0.7519515    1.37909553]\n",
      "Iteration:  71 Loss: 1997.2545000256005\n",
      "[ -3.23362968   0.28961461  -2.63363317   1.37882433 -10.99401851\n",
      "   2.8191892    1.24751576   0.75180363   1.37882433]\n",
      "Iteration:  72 Loss: 1996.4934931141743\n",
      "[ -3.23299378   0.28955762  -2.63311526   1.37855318 -10.9918565\n",
      "   2.8186348    1.24727043   0.75165578   1.37855318]\n",
      "Iteration:  73 Loss: 1995.7327854818204\n",
      "[ -3.232358     0.28950065  -2.63259745   1.37828208 -10.98969492\n",
      "   2.81808051   1.24702515   0.75150797   1.37828208]\n",
      "Iteration:  74 Loss: 1994.9723770108433\n",
      "[ -3.23172235   0.28944369  -2.63207974   1.37801104 -10.98753376\n",
      "   2.81752632   1.24677992   0.75136018   1.37801104]\n",
      "Iteration:  75 Loss: 1994.2122675835917\n",
      "[ -3.23108682   0.28938674  -2.63156214   1.37774005 -10.98537303\n",
      "   2.81697225   1.24653474   0.75121243   1.37774005]\n",
      "Iteration:  76 Loss: 1993.452457082463\n",
      "[ -3.23045142   0.2893298   -2.63104463   1.37746911 -10.98321273\n",
      "   2.81641828   1.24628961   0.7510647    1.37746911]\n",
      "Iteration:  77 Loss: 1992.6929453898981\n",
      "[ -3.22981614   0.28927287  -2.63052723   1.37719823 -10.98105285\n",
      "   2.81586443   1.24604452   0.750917     1.37719823]\n",
      "Iteration:  78 Loss: 1991.9337323883854\n",
      "[ -3.22918099   0.28921596  -2.63000993   1.3769274  -10.97889339\n",
      "   2.81531068   1.24579948   0.75076933   1.3769274 ]\n",
      "Iteration:  79 Loss: 1991.17481796046\n",
      "[ -3.22854596   0.28915905  -2.62949273   1.37665662 -10.97673436\n",
      "   2.81475704   1.24555449   0.75062169   1.37665662]\n",
      "Iteration:  80 Loss: 1990.4162019887024\n",
      "[ -3.22791105   0.28910216  -2.62897563   1.3763859  -10.97457575\n",
      "   2.81420351   1.24530955   0.75047408   1.3763859 ]\n",
      "Iteration:  81 Loss: 1989.657884355739\n",
      "[ -3.22727628   0.28904527  -2.62845864   1.37611523 -10.97241757\n",
      "   2.81365009   1.24506466   0.75032649   1.37611523]\n",
      "Iteration:  82 Loss: 1988.8998649442444\n",
      "[ -3.22664162   0.2889884   -2.62794174   1.37584461 -10.97025982\n",
      "   2.81309678   1.24481981   0.75017894   1.37584461]\n",
      "Iteration:  83 Loss: 1988.142143636937\n",
      "[ -3.22600709   0.28893154  -2.62742495   1.37557405 -10.96810248\n",
      "   2.81254357   1.24457501   0.75003141   1.37557405]\n",
      "Iteration:  84 Loss: 1987.3847203165822\n",
      "[ -3.22537269   0.28887469  -2.62690826   1.37530354 -10.96594557\n",
      "   2.81199048   1.24433026   0.74988392   1.37530354]\n",
      "Iteration:  85 Loss: 1986.6275948659916\n",
      "[ -3.22473841   0.28881785  -2.62639167   1.37503308 -10.96378909\n",
      "   2.81143749   1.24408556   0.74973645   1.37503308]\n",
      "Iteration:  86 Loss: 1985.870767168022\n",
      "[ -3.22410426   0.28876102  -2.62587518   1.37476267 -10.96163303\n",
      "   2.81088461   1.24384091   0.74958901   1.37476267]\n",
      "Iteration:  87 Loss: 1985.1142371055782\n",
      "[ -3.22347023   0.28870421  -2.6253588    1.37449232 -10.95947739\n",
      "   2.81033185   1.2435963    0.74944161   1.37449232]\n",
      "Iteration:  88 Loss: 1984.3580045616097\n",
      "[ -3.22283632   0.2886474   -2.62484251   1.37422203 -10.95732218\n",
      "   2.80977919   1.24335175   0.74929423   1.37422203]\n",
      "Iteration:  89 Loss: 1983.6020694191118\n",
      "[ -3.22220254   0.28859061  -2.62432633   1.37395178 -10.95516739\n",
      "   2.80922664   1.24310724   0.74914687   1.37395178]\n",
      "Iteration:  90 Loss: 1982.8464315611268\n",
      "[ -3.22156889   0.28853383  -2.62381025   1.37368159 -10.95301302\n",
      "   2.80867419   1.24286278   0.74899955   1.37368159]\n",
      "Iteration:  91 Loss: 1982.0910908707417\n",
      "[ -3.22093536   0.28847706  -2.62329427   1.37341145 -10.95085908\n",
      "   2.80812186   1.24261837   0.74885226   1.37341145]\n",
      "Iteration:  92 Loss: 1981.3360472310906\n",
      "[ -3.22030195   0.2884203   -2.62277839   1.37314137 -10.94870556\n",
      "   2.80756963   1.242374     0.748705     1.37314137]\n",
      "Iteration:  93 Loss: 1980.5813005253533\n",
      "[ -3.21966867   0.28836355  -2.62226261   1.37287133 -10.94655247\n",
      "   2.80701752   1.24212968   0.74855776   1.37287133]\n",
      "Iteration:  94 Loss: 1979.8268506367558\n",
      "[ -3.21903551   0.28830681  -2.62174694   1.37260135 -10.9443998\n",
      "   2.80646551   1.24188542   0.74841055   1.37260135]\n",
      "Iteration:  95 Loss: 1979.0726974485688\n",
      "[ -3.21840248   0.28825008  -2.62123136   1.37233143 -10.94224755\n",
      "   2.80591361   1.2416412    0.74826338   1.37233143]\n",
      "Iteration:  96 Loss: 1978.3188408441101\n",
      "[ -3.21776957   0.28819337  -2.62071589   1.37206156 -10.94009573\n",
      "   2.80536182   1.24139702   0.74811623   1.37206156]\n",
      "Iteration:  97 Loss: 1977.5652807067436\n",
      "[ -3.21713679   0.28813666  -2.62020052   1.37179174 -10.93794432\n",
      "   2.80481014   1.2411529    0.74796911   1.37179174]\n",
      "Iteration:  98 Loss: 1976.8120169198767\n",
      "[ -3.21650413   0.28807997  -2.61968525   1.37152197 -10.93579335\n",
      "   2.80425856   1.24090882   0.74782202   1.37152197]\n",
      "Iteration:  99 Loss: 1976.0590493669674\n",
      "[ -3.21587159   0.28802329  -2.61917008   1.37125226 -10.93364279\n",
      "   2.8037071    1.24066479   0.74767496   1.37125226]\n",
      "Iteration: 100 Loss: 1975.306377931514\n",
      "[ -3.21523918   0.28796662  -2.61865501   1.37098259 -10.93149266\n",
      "   2.80315574   1.24042081   0.74752793   1.37098259]\n",
      "Iteration: 101 Loss: 1974.5540024970646\n",
      "[ -3.2146069    0.28790996  -2.61814005   1.37071299 -10.92934295\n",
      "   2.80260449   1.24017688   0.74738092   1.37071299]\n",
      "Iteration: 102 Loss: 1973.8019229472109\n",
      "[ -3.21397474   0.28785331  -2.61762518   1.37044343 -10.92719366\n",
      "   2.80205335   1.239933     0.74723395   1.37044343]\n",
      "Iteration: 103 Loss: 1973.0501391655914\n",
      "[ -3.2133427    0.28779667  -2.61711042   1.37017393 -10.9250448\n",
      "   2.80150232   1.23968916   0.747087     1.37017393]\n",
      "Iteration: 104 Loss: 1972.2986510358905\n",
      "[ -3.21271079   0.28774004  -2.61659576   1.36990448 -10.92289636\n",
      "   2.80095139   1.23944537   0.74694009   1.36990448]\n",
      "Iteration: 105 Loss: 1971.5474584418382\n",
      "[ -3.212079     0.28768343  -2.6160812    1.36963509 -10.92074834\n",
      "   2.80040058   1.23920163   0.7467932    1.36963509]\n",
      "Iteration: 106 Loss: 1970.7965612672094\n",
      "[ -3.21144733   0.28762682  -2.61556674   1.36936574 -10.91860074\n",
      "   2.79984987   1.23895794   0.74664634   1.36936574]\n",
      "Iteration: 107 Loss: 1970.0459593958255\n",
      "[ -3.21081579   0.28757023  -2.61505238   1.36909645 -10.91645357\n",
      "   2.79929927   1.23871429   0.74649951   1.36909645]\n",
      "Iteration: 108 Loss: 1969.2956527115532\n",
      "[ -3.21018438   0.28751365  -2.61453812   1.36882722 -10.91430681\n",
      "   2.79874878   1.2384707    0.74635271   1.36882722]\n",
      "Iteration: 109 Loss: 1968.5456410983056\n",
      "[ -3.20955308   0.28745708  -2.61402396   1.36855803 -10.91216048\n",
      "   2.7981984    1.23822715   0.74620593   1.36855803]\n",
      "Iteration: 110 Loss: 1967.795924440041\n",
      "[ -3.20892192   0.28740052  -2.61350991   1.3682889  -10.91001458\n",
      "   2.79764813   1.23798365   0.74605919   1.3682889 ]\n",
      "Iteration: 111 Loss: 1967.0465026207623\n",
      "[ -3.20829087   0.28734397  -2.61299595   1.36801982 -10.90786909\n",
      "   2.79709796   1.2377402    0.74591248   1.36801982]\n",
      "Iteration: 112 Loss: 1966.2973755245202\n",
      "[ -3.20765995   0.28728743  -2.6124821    1.3677508  -10.90572403\n",
      "   2.7965479    1.23749679   0.74576579   1.3677508 ]\n",
      "Iteration: 113 Loss: 1965.548543035409\n",
      "[ -3.20702916   0.28723091  -2.61196835   1.36748183 -10.90357938\n",
      "   2.79599796   1.23725343   0.74561913   1.36748183]\n",
      "Iteration: 114 Loss: 1964.8000050375695\n",
      "[ -3.20639848   0.28717439  -2.6114547    1.36721291 -10.90143516\n",
      "   2.79544811   1.23701012   0.74547251   1.36721291]\n",
      "Iteration: 115 Loss: 1964.0517614151881\n",
      "[ -3.20576794   0.28711789  -2.61094115   1.36694404 -10.89929136\n",
      "   2.79489838   1.23676686   0.74532591   1.36694404]\n",
      "Iteration: 116 Loss: 1963.3038120524966\n",
      "[ -3.20513751   0.28706139  -2.6104277    1.36667523 -10.89714799\n",
      "   2.79434876   1.23652365   0.74517934   1.36667523]\n",
      "Iteration: 117 Loss: 1962.5561568337723\n",
      "[ -3.20450721   0.28700491  -2.60991435   1.36640647 -10.89500503\n",
      "   2.79379924   1.23628048   0.74503279   1.36640647]\n",
      "Iteration: 118 Loss: 1961.8087956433378\n",
      "[ -3.20387704   0.28694844  -2.6094011    1.36613776 -10.8928625\n",
      "   2.79324983   1.23603736   0.74488628   1.36613776]\n",
      "Iteration: 119 Loss: 1961.0617283655615\n",
      "[ -3.20324699   0.28689198  -2.60888796   1.3658691  -10.89072038\n",
      "   2.79270053   1.23579429   0.7447398    1.3658691 ]\n",
      "Iteration: 120 Loss: 1960.3149548848578\n",
      "[ -3.20261706   0.28683553  -2.60837491   1.3656005  -10.88857869\n",
      "   2.79215134   1.23555127   0.74459334   1.3656005 ]\n",
      "Iteration: 121 Loss: 1959.568475085685\n",
      "[ -3.20198726   0.2867791   -2.60786197   1.36533195 -10.88643742\n",
      "   2.79160225   1.2353083    0.74444692   1.36533195]\n",
      "Iteration: 122 Loss: 1958.8222888525481\n",
      "[ -3.20135758   0.28672267  -2.60734912   1.36506346 -10.88429657\n",
      "   2.79105328   1.23506537   0.74430052   1.36506346]\n",
      "Iteration: 123 Loss: 1958.0763960699971\n",
      "[ -3.20072802   0.28666625  -2.60683638   1.36479501 -10.88215614\n",
      "   2.79050441   1.23482249   0.74415415   1.36479501]\n",
      "Iteration: 124 Loss: 1957.3307966226284\n",
      "[ -3.20009859   0.28660985  -2.60632374   1.36452662 -10.88001614\n",
      "   2.78995565   1.23457966   0.74400781   1.36452662]\n",
      "Iteration: 125 Loss: 1956.585490395082\n",
      "[ -3.19946928   0.28655346  -2.6058112    1.36425828 -10.87787655\n",
      "   2.78940699   1.23433688   0.7438615    1.36425828]\n",
      "Iteration: 126 Loss: 1955.8404772720437\n",
      "[ -3.19884009   0.28649708  -2.60529876   1.36399    -10.87573738\n",
      "   2.78885845   1.23409414   0.74371522   1.36399   ]\n",
      "Iteration: 127 Loss: 1955.0957571382462\n",
      "[ -3.19821103   0.2864407   -2.60478642   1.36372177 -10.87359864\n",
      "   2.78831001   1.23385145   0.74356896   1.36372177]\n",
      "Iteration: 128 Loss: 1954.3513298784655\n",
      "[ -3.19758209   0.28638434  -2.60427418   1.36345359 -10.87146031\n",
      "   2.78776168   1.23360881   0.74342274   1.36345359]\n",
      "Iteration: 129 Loss: 1953.607195377525\n",
      "[ -3.19695328   0.286328    -2.60376204   1.36318546 -10.86932241\n",
      "   2.78721346   1.23336622   0.74327654   1.36318546]\n",
      "Iteration: 130 Loss: 1952.863353520291\n",
      "[ -3.19632459   0.28627166  -2.60325      1.36291738 -10.86718493\n",
      "   2.78666535   1.23312367   0.74313037   1.36291738]\n",
      "Iteration: 131 Loss: 1952.119804191677\n",
      "[ -3.19569602   0.28621533  -2.60273807   1.36264936 -10.86504786\n",
      "   2.78611734   1.23288118   0.74298423   1.36264936]\n",
      "Iteration: 132 Loss: 1951.3765472766413\n",
      "[ -3.19506758   0.28615902  -2.60222623   1.36238139 -10.86291122\n",
      "   2.78556944   1.23263873   0.74283812   1.36238139]\n",
      "Iteration: 133 Loss: 1950.6335826601864\n",
      "[ -3.19443926   0.28610271  -2.6017145    1.36211348 -10.860775\n",
      "   2.78502165   1.23239632   0.74269204   1.36211348]\n",
      "Iteration: 134 Loss: 1949.8909102273617\n",
      "[ -3.19381107   0.28604642  -2.60120286   1.36184561 -10.8586392\n",
      "   2.78447397   1.23215397   0.74254599   1.36184561]\n",
      "Iteration: 135 Loss: 1949.14852986326\n",
      "[ -3.19318299   0.28599014  -2.60069133   1.3615778  -10.85650381\n",
      "   2.7839264    1.23191166   0.74239997   1.3615778 ]\n",
      "Iteration: 136 Loss: 1948.4064414530221\n",
      "[ -3.19255505   0.28593386  -2.60017989   1.36131005 -10.85436885\n",
      "   2.78337893   1.2316694    0.74225397   1.36131005]\n",
      "Iteration: 137 Loss: 1947.6646448818294\n",
      "[ -3.19192722   0.2858776   -2.59966856   1.36104234 -10.85223431\n",
      "   2.78283157   1.23142719   0.742108     1.36104234]\n",
      "Iteration: 138 Loss: 1946.9231400349136\n",
      "[ -3.19129952   0.28582136  -2.59915733   1.36077469 -10.85010018\n",
      "   2.78228432   1.23118503   0.74196207   1.36077469]\n",
      "Iteration: 139 Loss: 1946.1819267975475\n",
      "[ -3.19067194   0.28576512  -2.5986462    1.36050709 -10.84796648\n",
      "   2.78173717   1.23094291   0.74181616   1.36050709]\n",
      "Iteration: 140 Loss: 1945.4410050550514\n",
      "[ -3.19004449   0.28570889  -2.59813517   1.36023954 -10.8458332\n",
      "   2.78119014   1.23070084   0.74167028   1.36023954]\n",
      "Iteration: 141 Loss: 1944.7003746927903\n",
      "[ -3.18941715   0.28565268  -2.59762423   1.35997204 -10.84370033\n",
      "   2.78064321   1.23045882   0.74152443   1.35997204]\n",
      "Iteration: 142 Loss: 1943.9600355961727\n",
      "[ -3.18878995   0.28559647  -2.5971134    1.3597046  -10.84156789\n",
      "   2.78009639   1.23021685   0.7413786    1.3597046 ]\n",
      "Iteration: 143 Loss: 1943.2199876506543\n",
      "[ -3.18816286   0.28554028  -2.59660267   1.35943721 -10.83943586\n",
      "   2.77954967   1.22997492   0.74123281   1.35943721]\n",
      "Iteration: 144 Loss: 1942.4802307417353\n",
      "[ -3.1875359    0.28548409  -2.59609205   1.35916987 -10.83730425\n",
      "   2.77900307   1.22973305   0.74108704   1.35916987]\n",
      "Iteration: 145 Loss: 1941.7407647549587\n",
      "[ -3.18690906   0.28542792  -2.59558152   1.35890259 -10.83517307\n",
      "   2.77845657   1.22949122   0.74094131   1.35890259]\n",
      "Iteration: 146 Loss: 1941.0015895759166\n",
      "[ -3.18628235   0.28537176  -2.59507109   1.35863536 -10.8330423\n",
      "   2.77791018   1.22924943   0.7407956    1.35863536]\n",
      "Iteration: 147 Loss: 1940.2627050902424\n",
      "[ -3.18565576   0.28531561  -2.59456076   1.35836818 -10.83091195\n",
      "   2.77736389   1.2290077    0.74064992   1.35836818]\n",
      "Iteration: 148 Loss: 1939.5241111836162\n",
      "[ -3.18502929   0.28525947  -2.59405053   1.35810105 -10.82878202\n",
      "   2.77681772   1.22876601   0.74050427   1.35810105]\n",
      "Iteration: 149 Loss: 1938.785807741763\n",
      "[ -3.18440294   0.28520335  -2.5935404    1.35783398 -10.82665251\n",
      "   2.77627165   1.22852437   0.74035865   1.35783398]\n",
      "Iteration: 150 Loss: 1938.0477946504527\n",
      "[ -3.18377672   0.28514723  -2.59303038   1.35756695 -10.82452342\n",
      "   2.77572568   1.22828278   0.74021305   1.35756695]\n",
      "Iteration: 151 Loss: 1937.3100717954994\n",
      "[ -3.18315062   0.28509112  -2.59252045   1.35729999 -10.82239474\n",
      "   2.77517983   1.22804123   0.74006749   1.35729999]\n",
      "Iteration: 152 Loss: 1936.5726390627628\n",
      "[ -3.18252464   0.28503503  -2.59201062   1.35703307 -10.82026649\n",
      "   2.77463408   1.22779973   0.73992195   1.35703307]\n",
      "Iteration: 153 Loss: 1935.8354963381482\n",
      "[ -3.18189879   0.28497895  -2.5915009    1.3567662  -10.81813865\n",
      "   2.77408844   1.22755828   0.73977644   1.3567662 ]\n",
      "Iteration: 154 Loss: 1935.0986435076036\n",
      "[ -3.18127306   0.28492287  -2.59099127   1.35649939 -10.81601123\n",
      "   2.77354291   1.22731688   0.73963096   1.35649939]\n",
      "Iteration: 155 Loss: 1934.3620804571242\n",
      "[ -3.18064746   0.28486681  -2.59048174   1.35623263 -10.81388423\n",
      "   2.77299748   1.22707552   0.73948551   1.35623263]\n",
      "Iteration: 156 Loss: 1933.6258070727479\n",
      "[ -3.18002197   0.28481076  -2.58997232   1.35596593 -10.81175765\n",
      "   2.77245217   1.22683422   0.73934009   1.35596593]\n",
      "Iteration: 157 Loss: 1932.8898232405602\n",
      "[ -3.17939661   0.28475472  -2.58946299   1.35569927 -10.80963149\n",
      "   2.77190696   1.22659296   0.7391947    1.35569927]\n",
      "Iteration: 158 Loss: 1932.1541288466885\n",
      "[ -3.17877137   0.2846987   -2.58895377   1.35543267 -10.80750574\n",
      "   2.77136185   1.22635174   0.73904933   1.35543267]\n",
      "Iteration: 159 Loss: 1931.4187237773058\n",
      "[ -3.17814626   0.28464268  -2.58844464   1.35516612 -10.80538042\n",
      "   2.77081686   1.22611058   0.738904     1.35516612]\n",
      "Iteration: 160 Loss: 1930.683607918631\n",
      "[ -3.17752127   0.28458667  -2.58793562   1.35489962 -10.80325551\n",
      "   2.77027197   1.22586946   0.73875869   1.35489962]\n",
      "Iteration: 161 Loss: 1929.9487811569272\n",
      "[ -3.1768964    0.28453068  -2.58742669   1.35463318 -10.80113101\n",
      "   2.76972719   1.22562839   0.73861341   1.35463318]\n",
      "Iteration: 162 Loss: 1929.2142433785011\n",
      "[ -3.17627165   0.28447469  -2.58691787   1.35436678 -10.79900694\n",
      "   2.76918251   1.22538737   0.73846816   1.35436678]\n",
      "Iteration: 163 Loss: 1928.4799944697065\n",
      "[ -3.17564703   0.28441872  -2.58640914   1.35410044 -10.79688329\n",
      "   2.76863794   1.22514639   0.73832294   1.35410044]\n",
      "Iteration: 164 Loss: 1927.746034316939\n",
      "[ -3.17502253   0.28436276  -2.58590052   1.35383416 -10.79476005\n",
      "   2.76809348   1.22490546   0.73817774   1.35383416]\n",
      "Iteration: 165 Loss: 1927.0123628066415\n",
      "[ -3.17439815   0.28430681  -2.58539199   1.35356792 -10.79263723\n",
      "   2.76754913   1.22466458   0.73803258   1.35356792]\n",
      "Iteration: 166 Loss: 1926.2789798252995\n",
      "[ -3.1737739    0.28425087  -2.58488357   1.35330174 -10.79051482\n",
      "   2.76700488   1.22442375   0.73788744   1.35330174]\n",
      "Iteration: 167 Loss: 1925.5458852594443\n",
      "[ -3.17314977   0.28419494  -2.58437524   1.35303561 -10.78839284\n",
      "   2.76646074   1.22418296   0.73774234   1.35303561]\n",
      "Iteration: 168 Loss: 1924.8130789956515\n",
      "[ -3.17252576   0.28413902  -2.58386702   1.35276953 -10.78627127\n",
      "   2.76591671   1.22394222   0.73759726   1.35276953]\n",
      "Iteration: 169 Loss: 1924.080560920541\n",
      "[ -3.17190187   0.28408311  -2.58335889   1.3525035  -10.78415012\n",
      "   2.76537278   1.22370153   0.73745221   1.3525035 ]\n",
      "Iteration: 170 Loss: 1923.348330920779\n",
      "[ -3.17127811   0.28402722  -2.58285087   1.35223753 -10.78202938\n",
      "   2.76482897   1.22346088   0.73730718   1.35223753]\n",
      "Iteration: 171 Loss: 1922.6163888830731\n",
      "[ -3.17065447   0.28397133  -2.58234294   1.35197161 -10.77990907\n",
      "   2.76428525   1.22322029   0.73716219   1.35197161]\n",
      "Iteration: 172 Loss: 1921.8847346941786\n",
      "[ -3.17003095   0.28391546  -2.58183512   1.35170574 -10.77778917\n",
      "   2.76374165   1.22297974   0.73701723   1.35170574]\n",
      "Iteration: 173 Loss: 1921.153368240893\n",
      "[ -3.16940755   0.28385959  -2.58132739   1.35143992 -10.77566968\n",
      "   2.76319815   1.22273923   0.73687229   1.35143992]\n",
      "Iteration: 174 Loss: 1920.4222894100608\n",
      "[ -3.16878428   0.28380374  -2.58081977   1.35117416 -10.77355062\n",
      "   2.76265476   1.22249878   0.73672738   1.35117416]\n",
      "Iteration: 175 Loss: 1919.6914980885683\n",
      "[ -3.16816113   0.2837479   -2.58031224   1.35090844 -10.77143197\n",
      "   2.76211148   1.22225837   0.7365825    1.35090844]\n",
      "Iteration: 176 Loss: 1918.9609941633473\n",
      "[ -3.1675381    0.28369207  -2.57980482   1.35064278 -10.76931373\n",
      "   2.7615683    1.22201801   0.73643765   1.35064278]\n",
      "Iteration: 177 Loss: 1918.230777521375\n",
      "[ -3.1669152    0.28363625  -2.57929749   1.35037718 -10.76719592\n",
      "   2.76102523   1.2217777    0.73629283   1.35037718]\n",
      "Iteration: 178 Loss: 1917.5008480496715\n",
      "[ -3.16629241   0.28358044  -2.57879026   1.35011162 -10.76507852\n",
      "   2.76048227   1.22153743   0.73614803   1.35011162]\n",
      "Iteration: 179 Loss: 1916.7712056353037\n",
      "[ -3.16566975   0.28352464  -2.57828314   1.34984612 -10.76296153\n",
      "   2.75993941   1.22129721   0.73600327   1.34984612]\n",
      "Iteration: 180 Loss: 1916.0418501653799\n",
      "[ -3.16504721   0.28346886  -2.57777611   1.34958067 -10.76084496\n",
      "   2.75939666   1.22105704   0.73585853   1.34958067]\n",
      "Iteration: 181 Loss: 1915.3127815270543\n",
      "[ -3.1644248    0.28341308  -2.57726918   1.34931527 -10.75872881\n",
      "   2.75885402   1.22081692   0.73571382   1.34931527]\n",
      "Iteration: 182 Loss: 1914.583999607526\n",
      "[ -3.1638025    0.28335732  -2.57676236   1.34904992 -10.75661308\n",
      "   2.75831148   1.22057684   0.73556914   1.34904992]\n",
      "Iteration: 183 Loss: 1913.8555042940382\n",
      "[ -3.16318033   0.28330156  -2.57625563   1.34878463 -10.75449776\n",
      "   2.75776905   1.22033681   0.73542449   1.34878463]\n",
      "Iteration: 184 Loss: 1913.1272954738774\n",
      "[ -3.16255829   0.28324582  -2.575749     1.34851938 -10.75238286\n",
      "   2.75722673   1.22009683   0.73527987   1.34851938]\n",
      "Iteration: 185 Loss: 1912.3993730343757\n",
      "[ -3.16193636   0.28319009  -2.57524247   1.34825419 -10.75026837\n",
      "   2.75668451   1.21985689   0.73513527   1.34825419]\n",
      "Iteration: 186 Loss: 1911.671736862908\n",
      "[ -3.16131456   0.28313437  -2.57473604   1.34798906 -10.7481543\n",
      "   2.7561424    1.219617     0.7349907    1.34798906]\n",
      "Iteration: 187 Loss: 1910.9443868468954\n",
      "[ -3.16069287   0.28307866  -2.57422971   1.34772397 -10.74604064\n",
      "   2.7556004    1.21937716   0.73484617   1.34772397]\n",
      "Iteration: 188 Loss: 1910.2173228738022\n",
      "[ -3.16007131   0.28302296  -2.57372348   1.34745894 -10.7439274\n",
      "   2.7550585    1.21913737   0.73470166   1.34745894]\n",
      "Iteration: 189 Loss: 1909.4905448311376\n",
      "[ -3.15944988   0.28296727  -2.57321735   1.34719395 -10.74181458\n",
      "   2.75451671   1.21889762   0.73455718   1.34719395]\n",
      "Iteration: 190 Loss: 1908.7640526064533\n",
      "[ -3.15882856   0.2829116   -2.57271132   1.34692902 -10.73970217\n",
      "   2.75397503   1.21865792   0.73441272   1.34692902]\n",
      "Iteration: 191 Loss: 1908.0378460873474\n",
      "[ -3.15820737   0.28285593  -2.57220539   1.34666415 -10.73759018\n",
      "   2.75343345   1.21841827   0.7342683    1.34666415]\n",
      "Iteration: 192 Loss: 1907.311925161461\n",
      "[ -3.1575863    0.28280028  -2.57169956   1.34639932 -10.7354786\n",
      "   2.75289198   1.21817866   0.7341239    1.34639932]\n",
      "Iteration: 193 Loss: 1906.5862897164795\n",
      "[ -3.15696535   0.28274463  -2.57119383   1.34613455 -10.73336743\n",
      "   2.75235061   1.2179391    0.73397954   1.34613455]\n",
      "Iteration: 194 Loss: 1905.8609396401323\n",
      "[ -3.15634452   0.282689    -2.5706882    1.34586983 -10.73125669\n",
      "   2.75180936   1.21769959   0.7338352    1.34586983]\n",
      "Iteration: 195 Loss: 1905.1358748201942\n",
      "[ -3.15572382   0.28263338  -2.57018266   1.34560516 -10.72914635\n",
      "   2.7512682    1.21746013   0.73369089   1.34560516]\n",
      "Iteration: 196 Loss: 1904.4110951444823\n",
      "[ -3.15510324   0.28257777  -2.56967723   1.34534054 -10.72703644\n",
      "   2.75072716   1.21722071   0.7335466    1.34534054]\n",
      "Iteration: 197 Loss: 1903.6866005008594\n",
      "[ -3.15448278   0.28252217  -2.56917189   1.34507598 -10.72492693\n",
      "   2.75018622   1.21698134   0.73340235   1.34507598]\n",
      "Iteration: 198 Loss: 1902.9623907772318\n",
      "[ -3.15386244   0.28246658  -2.56866666   1.34481146 -10.72281785\n",
      "   2.74964539   1.21674202   0.73325812   1.34481146]\n",
      "Iteration: 199 Loss: 1902.2384658615488\n",
      "[ -3.15324222   0.282411    -2.56816152   1.344547   -10.72070917\n",
      "   2.74910466   1.21650274   0.73311393   1.344547  ]\n",
      "Iteration: 200 Loss: 1901.514825641806\n",
      "[ -3.15262213   0.28235543  -2.56765649   1.34428259 -10.71860091\n",
      "   2.74856404   1.21626351   0.73296976   1.34428259]\n",
      "Iteration: 201 Loss: 1900.791470006041\n",
      "[ -3.15200216   0.28229988  -2.56715155   1.34401824 -10.71649307\n",
      "   2.74802353   1.21602433   0.73282562   1.34401824]\n",
      "Iteration: 202 Loss: 1900.0683988423366\n",
      "[ -3.15138231   0.28224433  -2.56664671   1.34375393 -10.71438564\n",
      "   2.74748312   1.2157852    0.7326815    1.34375393]\n",
      "Iteration: 203 Loss: 1899.345612038819\n",
      "[ -3.15076258   0.2821888   -2.56614197   1.34348968 -10.71227863\n",
      "   2.74694282   1.21554611   0.73253742   1.34348968]\n",
      "Iteration: 204 Loss: 1898.6231094836587\n",
      "[ -3.15014297   0.28213327  -2.56563733   1.34322548 -10.71017203\n",
      "   2.74640263   1.21530707   0.73239336   1.34322548]\n",
      "Iteration: 205 Loss: 1897.9008910650707\n",
      "[ -3.14952349   0.28207776  -2.56513279   1.34296133 -10.70806584\n",
      "   2.74586254   1.21506808   0.73224934   1.34296133]\n",
      "Iteration: 206 Loss: 1897.1789566713128\n",
      "[ -3.14890412   0.28202226  -2.56462835   1.34269723 -10.70596007\n",
      "   2.74532256   1.21482913   0.73210534   1.34269723]\n",
      "Iteration: 207 Loss: 1896.4573061906874\n",
      "[ -3.14828488   0.28196677  -2.56412401   1.34243318 -10.70385471\n",
      "   2.74478268   1.21459023   0.73196137   1.34243318]\n",
      "Iteration: 208 Loss: 1895.7359395115411\n",
      "[ -3.14766576   0.28191129  -2.56361977   1.34216919 -10.70174977\n",
      "   2.74424291   1.21435138   0.73181743   1.34216919]\n",
      "Iteration: 209 Loss: 1895.0148565222637\n",
      "[ -3.14704677   0.28185582  -2.56311563   1.34190525 -10.69964524\n",
      "   2.74370325   1.21411257   0.73167351   1.34190525]\n",
      "Iteration: 210 Loss: 1894.2940571112888\n",
      "[ -3.14642789   0.28180036  -2.56261158   1.34164136 -10.69754112\n",
      "   2.74316369   1.21387381   0.73152963   1.34164136]\n",
      "Iteration: 211 Loss: 1893.5735411670958\n",
      "[ -3.14580914   0.28174491  -2.56210764   1.34137752 -10.69543742\n",
      "   2.74262424   1.2136351    0.73138577   1.34137752]\n",
      "Iteration: 212 Loss: 1892.8533085782053\n",
      "[ -3.1451905    0.28168948  -2.56160379   1.34111374 -10.69333413\n",
      "   2.7420849    1.21339644   0.73124194   1.34111374]\n",
      "Iteration: 213 Loss: 1892.1333592331835\n",
      "[ -3.14457199   0.28163405  -2.56110004   1.34085    -10.69123125\n",
      "   2.74154566   1.21315782   0.73109814   1.34085   ]\n",
      "Iteration: 214 Loss: 1891.4136930206396\n",
      "[ -3.1439536    0.28157864  -2.5605964    1.34058632 -10.68912879\n",
      "   2.74100653   1.21291925   0.73095437   1.34058632]\n",
      "Iteration: 215 Loss: 1890.6943098292272\n",
      "[ -3.14333534   0.28152323  -2.56009285   1.34032269 -10.68702674\n",
      "   2.7404675    1.21268072   0.73081062   1.34032269]\n",
      "Iteration: 216 Loss: 1889.9752095476429\n",
      "[ -3.14271719   0.28146784  -2.5595894    1.34005911 -10.68492511\n",
      "   2.73992858   1.21244225   0.73066691   1.34005911]\n",
      "Iteration: 217 Loss: 1889.2563920646278\n",
      "[ -3.14209917   0.28141246  -2.55908605   1.33979559 -10.68282389\n",
      "   2.73938976   1.21220382   0.73052322   1.33979559]\n",
      "Iteration: 218 Loss: 1888.5378572689667\n",
      "[ -3.14148126   0.28135709  -2.5585828    1.33953211 -10.68072308\n",
      "   2.73885105   1.21196543   0.73037956   1.33953211]\n",
      "Iteration: 219 Loss: 1887.8196050494878\n",
      "[ -3.14086348   0.28130173  -2.55807964   1.33926869 -10.67862268\n",
      "   2.73831245   1.2117271    0.73023593   1.33926869]\n",
      "Iteration: 220 Loss: 1887.1016352950628\n",
      "[ -3.14024582   0.28124638  -2.55757659   1.33900532 -10.6765227\n",
      "   2.73777395   1.21148881   0.73009232   1.33900532]\n",
      "Iteration: 221 Loss: 1886.383947894608\n",
      "[ -3.13962828   0.28119104  -2.55707364   1.338742   -10.67442313\n",
      "   2.73723556   1.21125056   0.72994875   1.338742  ]\n",
      "Iteration: 222 Loss: 1885.6665427370824\n",
      "[ -3.13901087   0.28113571  -2.55657078   1.33847873 -10.67232398\n",
      "   2.73669728   1.21101237   0.7298052    1.33847873]\n",
      "Iteration: 223 Loss: 1884.9494197114886\n",
      "[ -3.13839357   0.2810804   -2.55606802   1.33821551 -10.67022523\n",
      "   2.7361591    1.21077422   0.72966168   1.33821551]\n",
      "Iteration: 224 Loss: 1884.2325787068748\n",
      "[ -3.1377764    0.28102509  -2.55556536   1.33795235 -10.6681269\n",
      "   2.73562103   1.21053612   0.72951819   1.33795235]\n",
      "Iteration: 225 Loss: 1883.5160196123302\n",
      "[ -3.13715934   0.2809698   -2.5550628    1.33768924 -10.66602899\n",
      "   2.73508306   1.21029806   0.72937473   1.33768924]\n",
      "Iteration: 226 Loss: 1882.7997423169895\n",
      "[ -3.13654241   0.28091451  -2.55456034   1.33742618 -10.66393148\n",
      "   2.7345452    1.21006005   0.7292313    1.33742618]\n",
      "Iteration: 227 Loss: 1882.0837467100303\n",
      "[ -3.1359256    0.28085924  -2.55405798   1.33716317 -10.66183439\n",
      "   2.73400744   1.20982209   0.72908789   1.33716317]\n",
      "Iteration: 228 Loss: 1881.3680326806732\n",
      "[ -3.13530891   0.28080398  -2.55355572   1.33690021 -10.65973771\n",
      "   2.73346979   1.20958417   0.72894452   1.33690021]\n",
      "Iteration: 229 Loss: 1880.652600118183\n",
      "[ -3.13469235   0.28074872  -2.55305356   1.33663731 -10.65764144\n",
      "   2.73293224   1.20934631   0.72880117   1.33663731]\n",
      "Iteration: 230 Loss: 1879.9374489118684\n",
      "[ -3.1340759    0.28069348  -2.55255149   1.33637445 -10.65554558\n",
      "   2.73239481   1.20910849   0.72865785   1.33637445]\n",
      "Iteration: 231 Loss: 1879.222578951082\n",
      "[ -3.13345957   0.28063825  -2.55204952   1.33611165 -10.65345014\n",
      "   2.73185747   1.20887071   0.72851455   1.33611165]\n",
      "Iteration: 232 Loss: 1878.5079901252172\n",
      "[ -3.13284337   0.28058304  -2.55154766   1.3358489  -10.65135511\n",
      "   2.73132024   1.20863298   0.72837129   1.3358489 ]\n",
      "Iteration: 233 Loss: 1877.7936823237144\n",
      "[ -3.13222729   0.28052783  -2.55104589   1.3355862  -10.64926049\n",
      "   2.73078312   1.2083953    0.72822805   1.3355862 ]\n",
      "Iteration: 234 Loss: 1877.0796554360552\n",
      "[ -3.13161133   0.28047263  -2.55054422   1.33532356 -10.64716628\n",
      "   2.73024611   1.20815767   0.72808484   1.33532356]\n",
      "Iteration: 235 Loss: 1876.3659093517658\n",
      "[ -3.13099549   0.28041745  -2.55004264   1.33506096 -10.64507248\n",
      "   2.7297092    1.20792008   0.72794166   1.33506096]\n",
      "Iteration: 236 Loss: 1875.6524439604152\n",
      "[ -3.13037977   0.28036227  -2.54954117   1.33479842 -10.6429791\n",
      "   2.72917239   1.20768254   0.72779851   1.33479842]\n",
      "Iteration: 237 Loss: 1874.9392591516162\n",
      "[ -3.12976417   0.28030711  -2.5490398    1.33453593 -10.64088613\n",
      "   2.72863569   1.20744504   0.72765539   1.33453593]\n",
      "Iteration: 238 Loss: 1874.226354815025\n",
      "[ -3.12914869   0.28025195  -2.54853852   1.33427349 -10.63879357\n",
      "   2.7280991    1.2072076    0.72751229   1.33427349]\n",
      "Iteration: 239 Loss: 1873.5137308403412\n",
      "[ -3.12853333   0.28019681  -2.54803734   1.3340111  -10.63670142\n",
      "   2.72756261   1.2069702    0.72736923   1.3340111 ]\n",
      "Iteration: 240 Loss: 1872.8013871173068\n",
      "[ -3.1279181    0.28014168  -2.54753626   1.33374876 -10.63460968\n",
      "   2.72702622   1.20673284   0.72722619   1.33374876]\n",
      "Iteration: 241 Loss: 1872.089323535709\n",
      "[ -3.12730299   0.28008656  -2.54703528   1.33348647 -10.63251835\n",
      "   2.72648995   1.20649553   0.72708318   1.33348647]\n",
      "Iteration: 242 Loss: 1871.3775399853769\n",
      "[ -3.12668799   0.28003145  -2.5465344    1.33322424 -10.63042744\n",
      "   2.72595377   1.20625827   0.72694019   1.33322424]\n",
      "Iteration: 243 Loss: 1870.6660363561828\n",
      "[ -3.12607312   0.27997635  -2.54603362   1.33296206 -10.62833693\n",
      "   2.72541771   1.20602106   0.72679724   1.33296206]\n",
      "Iteration: 244 Loss: 1869.954812538044\n",
      "[ -3.12545837   0.27992126  -2.54553293   1.33269993 -10.62624684\n",
      "   2.72488175   1.20578389   0.72665431   1.33269993]\n",
      "Iteration: 245 Loss: 1869.2438684209196\n",
      "[ -3.12484374   0.27986618  -2.54503235   1.33243785 -10.62415716\n",
      "   2.72434589   1.20554677   0.72651141   1.33243785]\n",
      "Iteration: 246 Loss: 1868.533203894812\n",
      "[ -3.12422923   0.27981111  -2.54453186   1.33217582 -10.62206789\n",
      "   2.72381014   1.2053097    0.72636854   1.33217582]\n",
      "Iteration: 247 Loss: 1867.8228188497674\n",
      "[ -3.12361484   0.27975606  -2.54403147   1.33191384 -10.61997903\n",
      "   2.7232745    1.20507267   0.7262257    1.33191384]\n",
      "Iteration: 248 Loss: 1867.1127131758758\n",
      "[ -3.12300057   0.27970101  -2.54353118   1.33165192 -10.61789058\n",
      "   2.72273896   1.20483569   0.72608288   1.33165192]\n",
      "Iteration: 249 Loss: 1866.4028867632683\n",
      "[ -3.12238643   0.27964598  -2.54303099   1.33139005 -10.61580254\n",
      "   2.72220352   1.20459875   0.7259401    1.33139005]\n",
      "Iteration: 250 Loss: 1865.693339502122\n",
      "[ -3.1217724    0.27959095  -2.54253089   1.33112822 -10.61371491\n",
      "   2.72166819   1.20436187   0.72579734   1.33112822]\n",
      "Iteration: 251 Loss: 1864.984071282654\n",
      "[ -3.12115849   0.27953594  -2.5420309    1.33086645 -10.61162769\n",
      "   2.72113297   1.20412502   0.72565461   1.33086645]\n",
      "Iteration: 252 Loss: 1864.2750819951282\n",
      "[ -3.12054471   0.27948094  -2.541531     1.33060473 -10.60954088\n",
      "   2.72059785   1.20388823   0.72551191   1.33060473]\n",
      "Iteration: 253 Loss: 1863.5663715298488\n",
      "[ -3.11993104   0.27942595  -2.5410312    1.33034307 -10.60745449\n",
      "   2.72006284   1.20365148   0.72536923   1.33034307]\n",
      "Iteration: 254 Loss: 1862.8579397771646\n",
      "[ -3.1193175    0.27937097  -2.5405315    1.33008145 -10.6053685\n",
      "   2.71952793   1.20341478   0.72522659   1.33008145]\n",
      "Iteration: 255 Loss: 1862.1497866274672\n",
      "[ -3.11870408   0.279316    -2.54003189   1.32981989 -10.60328292\n",
      "   2.71899312   1.20317812   0.72508397   1.32981989]\n",
      "Iteration: 256 Loss: 1861.4419119711902\n",
      "[ -3.11809078   0.27926104  -2.53953239   1.32955837 -10.60119776\n",
      "   2.71845843   1.20294152   0.72494138   1.32955837]\n",
      "Iteration: 257 Loss: 1860.734315698812\n",
      "[ -3.11747759   0.27920609  -2.53903298   1.32929691 -10.599113\n",
      "   2.71792383   1.20270495   0.72479882   1.32929691]\n",
      "Iteration: 258 Loss: 1860.0269977008536\n",
      "[ -3.11686453   0.27915116  -2.53853368   1.3290355  -10.59702866\n",
      "   2.71738935   1.20246844   0.72465628   1.3290355 ]\n",
      "Iteration: 259 Loss: 1859.3199578678784\n",
      "[ -3.11625159   0.27909623  -2.53803447   1.32877414 -10.59494472\n",
      "   2.71685496   1.20223197   0.72451378   1.32877414]\n",
      "Iteration: 260 Loss: 1858.6131960904931\n",
      "[ -3.11563877   0.27904131  -2.53753535   1.32851284 -10.59286119\n",
      "   2.71632069   1.20199555   0.7243713    1.32851284]\n",
      "Iteration: 261 Loss: 1857.9067122593478\n",
      "[ -3.11502607   0.27898641  -2.53703634   1.32825158 -10.59077808\n",
      "   2.71578651   1.20175917   0.72422885   1.32825158]\n",
      "Iteration: 262 Loss: 1857.2005062651353\n",
      "[ -3.1144135    0.27893151  -2.53653742   1.32799038 -10.58869537\n",
      "   2.71525245   1.20152284   0.72408643   1.32799038]\n",
      "Iteration: 263 Loss: 1856.4945779985915\n",
      "[ -3.11380104   0.27887663  -2.53603861   1.32772922 -10.58661307\n",
      "   2.71471849   1.20128656   0.72394404   1.32772922]\n",
      "Iteration: 264 Loss: 1855.788927350495\n",
      "[ -3.1131887    0.27882176  -2.53553989   1.32746812 -10.58453118\n",
      "   2.71418463   1.20105032   0.72380167   1.32746812]\n",
      "Iteration: 265 Loss: 1855.0835542116677\n",
      "[ -3.11257648   0.2787669   -2.53504127   1.32720707 -10.5824497\n",
      "   2.71365088   1.20081413   0.72365933   1.32720707]\n",
      "Iteration: 266 Loss: 1854.3784584729747\n",
      "[ -3.11196438   0.27871205  -2.53454274   1.32694607 -10.58036864\n",
      "   2.71311723   1.20057799   0.72351702   1.32694607]\n",
      "Iteration: 267 Loss: 1853.6736400253224\n",
      "[ -3.11135241   0.27865721  -2.53404432   1.32668512 -10.57828798\n",
      "   2.71258369   1.20034189   0.72337474   1.32668512]\n",
      "Iteration: 268 Loss: 1852.969098759663\n",
      "[ -3.11074055   0.27860238  -2.53354599   1.32642423 -10.57620772\n",
      "   2.71205025   1.20010584   0.72323249   1.32642423]\n",
      "Iteration: 269 Loss: 1852.2648345669888\n",
      "[ -3.11012881   0.27854756  -2.53304776   1.32616338 -10.57412788\n",
      "   2.71151692   1.19986984   0.72309026   1.32616338]\n",
      "Iteration: 270 Loss: 1851.5608473383359\n",
      "[ -3.1095172    0.27849275  -2.53254963   1.32590259 -10.57204845\n",
      "   2.71098369   1.19963388   0.72294806   1.32590259]\n",
      "Iteration: 271 Loss: 1850.857136964784\n",
      "[ -3.1089057    0.27843796  -2.5320516    1.32564185 -10.56996943\n",
      "   2.71045057   1.19939797   0.72280589   1.32564185]\n",
      "Iteration: 272 Loss: 1850.1537033374555\n",
      "[ -3.10829433   0.27838317  -2.53155366   1.32538115 -10.56789081\n",
      "   2.70991755   1.1991621    0.72266375   1.32538115]\n",
      "Iteration: 273 Loss: 1849.450546347514\n",
      "[ -3.10768307   0.27832839  -2.53105582   1.32512051 -10.56581261\n",
      "   2.70938463   1.19892628   0.72252164   1.32512051]\n",
      "Iteration: 274 Loss: 1848.747665886168\n",
      "[ -3.10707194   0.27827363  -2.53055809   1.32485993 -10.56373481\n",
      "   2.70885183   1.19869051   0.72237955   1.32485993]\n",
      "Iteration: 275 Loss: 1848.045061844667\n",
      "[ -3.10646092   0.27821888  -2.53006044   1.32459939 -10.56165742\n",
      "   2.70831912   1.19845479   0.72223749   1.32459939]\n",
      "Iteration: 276 Loss: 1847.3427341143056\n",
      "[ -3.10585003   0.27816413  -2.5295629    1.3243389  -10.55958044\n",
      "   2.70778652   1.19821911   0.72209546   1.3243389 ]\n",
      "Iteration: 277 Loss: 1846.6406825864185\n",
      "[ -3.10523925   0.2781094   -2.52906545   1.32407847 -10.55750387\n",
      "   2.70725403   1.19798347   0.72195346   1.32407847]\n",
      "Iteration: 278 Loss: 1845.9389071523842\n",
      "[ -3.1046286    0.27805468  -2.52856811   1.32381808 -10.5554277\n",
      "   2.70672164   1.19774789   0.72181149   1.32381808]\n",
      "Iteration: 279 Loss: 1845.2374077036247\n",
      "[ -3.10401807   0.27799997  -2.52807086   1.32355775 -10.55335195\n",
      "   2.70618936   1.19751235   0.72166954   1.32355775]\n",
      "Iteration: 280 Loss: 1844.5361841316037\n",
      "[ -3.10340765   0.27794527  -2.5275737    1.32329747 -10.5512766\n",
      "   2.70565718   1.19727685   0.72152762   1.32329747]\n",
      "Iteration: 281 Loss: 1843.8352363278286\n",
      "[ -3.10279736   0.27789058  -2.52707665   1.32303724 -10.54920166\n",
      "   2.7051251    1.1970414    0.72138573   1.32303724]\n",
      "Iteration: 282 Loss: 1843.1345641838468\n",
      "[ -3.10218718   0.2778359   -2.52657969   1.32277706 -10.54712713\n",
      "   2.70459313   1.196806     0.72124387   1.32277706]\n",
      "Iteration: 283 Loss: 1842.4341675912533\n",
      "[ -3.10157713   0.27778123  -2.52608283   1.32251693 -10.54505301\n",
      "   2.70406126   1.19657065   0.72110204   1.32251693]\n",
      "Iteration: 284 Loss: 1841.7340464416804\n",
      "[ -3.1009672    0.27772658  -2.52558607   1.32225686 -10.54297929\n",
      "   2.7035295    1.19633534   0.72096023   1.32225686]\n",
      "Iteration: 285 Loss: 1841.0342006268063\n",
      "[ -3.10035738   0.27767193  -2.52508941   1.32199683 -10.54090598\n",
      "   2.70299785   1.19610007   0.72081845   1.32199683]\n",
      "Iteration: 286 Loss: 1840.3346300383505\n",
      "[ -3.09974769   0.2776173   -2.52459284   1.32173685 -10.53883309\n",
      "   2.70246629   1.19586486   0.7206767    1.32173685]\n",
      "Iteration: 287 Loss: 1839.635334568076\n",
      "[ -3.09913811   0.27756267  -2.52409637   1.32147693 -10.53676059\n",
      "   2.70193485   1.19562969   0.72053498   1.32147693]\n",
      "Iteration: 288 Loss: 1838.9363141077877\n",
      "[ -3.09852866   0.27750806  -2.5236       1.32121706 -10.53468851\n",
      "   2.7014035    1.19539456   0.72039328   1.32121706]\n",
      "Iteration: 289 Loss: 1838.2375685493328\n",
      "[ -3.09791932   0.27745345  -2.52310373   1.32095724 -10.53261683\n",
      "   2.70087226   1.19515949   0.72025161   1.32095724]\n",
      "Iteration: 290 Loss: 1837.5390977846018\n",
      "[ -3.09731011   0.27739886  -2.52260755   1.32069747 -10.53054556\n",
      "   2.70034113   1.19492445   0.72010997   1.32069747]\n",
      "Iteration: 291 Loss: 1836.8409017055271\n",
      "[ -3.09670101   0.27734428  -2.52211148   1.32043775 -10.5284747\n",
      "   2.6998101    1.19468947   0.71996836   1.32043775]\n",
      "Iteration: 292 Loss: 1836.1429802040846\n",
      "[ -3.09609204   0.27728971  -2.52161549   1.32017808 -10.52640425\n",
      "   2.69927917   1.19445453   0.71982678   1.32017808]\n",
      "Iteration: 293 Loss: 1835.4453331722905\n",
      "[ -3.09548318   0.27723515  -2.52111961   1.31991846 -10.5243342\n",
      "   2.69874835   1.19421964   0.71968522   1.31991846]\n",
      "Iteration: 294 Loss: 1834.7479605022058\n",
      "[ -3.09487445   0.2771806   -2.52062383   1.3196589  -10.52226456\n",
      "   2.69821764   1.19398479   0.71954369   1.3196589 ]\n",
      "Iteration: 295 Loss: 1834.0508620859332\n",
      "[ -3.09426583   0.27712606  -2.52012814   1.31939938 -10.52019532\n",
      "   2.69768702   1.19374999   0.71940219   1.31939938]\n",
      "Iteration: 296 Loss: 1833.3540378156172\n",
      "[ -3.09365734   0.27707153  -2.51963255   1.31913992 -10.5181265\n",
      "   2.69715652   1.19351524   0.71926072   1.31913992]\n",
      "Iteration: 297 Loss: 1832.6574875834453\n",
      "[ -3.09304896   0.27701702  -2.51913705   1.31888051 -10.51605808\n",
      "   2.69662611   1.19328053   0.71911928   1.31888051]\n",
      "Iteration: 298 Loss: 1831.961211281647\n",
      "[ -3.0924407    0.27696251  -2.51864166   1.31862115 -10.51399006\n",
      "   2.69609581   1.19304587   0.71897786   1.31862115]\n",
      "Iteration: 299 Loss: 1831.2652088024954\n",
      "[ -3.09183256   0.27690801  -2.51814636   1.31836184 -10.51192246\n",
      "   2.69556562   1.19281125   0.71883647   1.31836184]\n",
      "Iteration: 300 Loss: 1830.5694800383046\n",
      "[ -3.09122455   0.27685353  -2.51765116   1.31810258 -10.50985526\n",
      "   2.69503553   1.19257668   0.71869511   1.31810258]\n",
      "Iteration: 301 Loss: 1829.8740248814306\n",
      "[ -3.09061665   0.27679905  -2.51715606   1.31784337 -10.50778846\n",
      "   2.69450554   1.19234216   0.71855378   1.31784337]\n",
      "Iteration: 302 Loss: 1829.1788432242738\n",
      "[ -3.09000887   0.27674459  -2.51666105   1.31758421 -10.50572208\n",
      "   2.69397566   1.19210768   0.71841247   1.31758421]\n",
      "Iteration: 303 Loss: 1828.4839349592748\n",
      "[ -3.08940121   0.27669014  -2.51616614   1.3173251  -10.5036561\n",
      "   2.69344588   1.19187325   0.71827119   1.3173251 ]\n",
      "Iteration: 304 Loss: 1827.7892999789185\n",
      "[ -3.08879367   0.27663569  -2.51567133   1.31706605 -10.50159052\n",
      "   2.69291621   1.19163886   0.71812994   1.31706605]\n",
      "Iteration: 305 Loss: 1827.0949381757298\n",
      "[ -3.08818625   0.27658126  -2.51517662   1.31680704 -10.49952535\n",
      "   2.69238664   1.19140452   0.71798872   1.31680704]\n",
      "Iteration: 306 Loss: 1826.4008494422776\n",
      "[ -3.08757895   0.27652684  -2.514682     1.31654809 -10.49746059\n",
      "   2.69185717   1.19117023   0.71784752   1.31654809]\n",
      "Iteration: 307 Loss: 1825.707033671173\n",
      "[ -3.08697177   0.27647243  -2.51418748   1.31628918 -10.49539624\n",
      "   2.69132781   1.19093598   0.71770636   1.31628918]\n",
      "Iteration: 308 Loss: 1825.013490755068\n",
      "[ -3.08636471   0.27641803  -2.51369306   1.31603033 -10.49333229\n",
      "   2.69079855   1.19070178   0.71756522   1.31603033]\n",
      "Iteration: 309 Loss: 1824.3202205866585\n",
      "[ -3.08575776   0.27636364  -2.51319873   1.31577153 -10.49126874\n",
      "   2.6902694    1.19046763   0.71742411   1.31577153]\n",
      "Iteration: 310 Loss: 1823.6272230586812\n",
      "[ -3.08515094   0.27630927  -2.51270451   1.31551278 -10.4892056\n",
      "   2.68974035   1.19023352   0.71728302   1.31551278]\n",
      "Iteration: 311 Loss: 1822.9344980639153\n",
      "[ -3.08454424   0.2762549   -2.51221038   1.31525408 -10.48714287\n",
      "   2.6892114    1.18999945   0.71714197   1.31525408]\n",
      "Iteration: 312 Loss: 1822.2420454951834\n",
      "[ -3.08393765   0.27620054  -2.51171634   1.31499543 -10.48508054\n",
      "   2.68868256   1.18976544   0.71700094   1.31499543]\n",
      "Iteration: 313 Loss: 1821.5498652453482\n",
      "[ -3.08333119   0.2761462   -2.51122241   1.31473684 -10.48301862\n",
      "   2.68815383   1.18953147   0.71685994   1.31473684]\n",
      "Iteration: 314 Loss: 1820.8579572073165\n",
      "[ -3.08272484   0.27609186  -2.51072857   1.31447829 -10.48095711\n",
      "   2.68762519   1.18929754   0.71671897   1.31447829]\n",
      "Iteration: 315 Loss: 1820.1663212740357\n",
      "[ -3.08211861   0.27603754  -2.51023482   1.31421979 -10.478896\n",
      "   2.68709666   1.18906366   0.71657802   1.31421979]\n",
      "Iteration: 316 Loss: 1819.4749573384966\n",
      "[ -3.08151251   0.27598322  -2.50974118   1.31396135 -10.47683529\n",
      "   2.68656824   1.18882983   0.71643711   1.31396135]\n",
      "Iteration: 317 Loss: 1818.7838652937305\n",
      "[ -3.08090652   0.27592892  -2.50924763   1.31370295 -10.47477499\n",
      "   2.68603992   1.18859604   0.71629622   1.31370295]\n",
      "Iteration: 318 Loss: 1818.0930450328126\n",
      "[ -3.08030065   0.27587463  -2.50875418   1.31344461 -10.4727151\n",
      "   2.6855117    1.1883623    0.71615535   1.31344461]\n",
      "Iteration: 319 Loss: 1817.402496448858\n",
      "[ -3.0796949    0.27582034  -2.50826083   1.31318632 -10.47065561\n",
      "   2.68498359   1.18812861   0.71601452   1.31318632]\n",
      "Iteration: 320 Loss: 1816.7122194350266\n",
      "[ -3.07908927   0.27576607  -2.50776757   1.31292808 -10.46859652\n",
      "   2.68445558   1.18789496   0.71587371   1.31292808]\n",
      "Iteration: 321 Loss: 1816.0222138845177\n",
      "[ -3.07848376   0.27571181  -2.50727441   1.31266988 -10.46653784\n",
      "   2.68392767   1.18766136   0.71573294   1.31266988]\n",
      "Iteration: 322 Loss: 1815.3324796905736\n",
      "[ -3.07787836   0.27565756  -2.50678135   1.31241174 -10.46447957\n",
      "   2.68339987   1.1874278    0.71559218   1.31241174]\n",
      "Iteration: 323 Loss: 1814.6430167464794\n",
      "[ -3.07727309   0.27560332  -2.50628838   1.31215365 -10.4624217\n",
      "   2.68287217   1.18719429   0.71545146   1.31215365]\n",
      "Iteration: 324 Loss: 1813.9538249455609\n",
      "[ -3.07666794   0.27554909  -2.50579551   1.31189562 -10.46036423\n",
      "   2.68234458   1.18696082   0.71531077   1.31189562]\n",
      "Iteration: 325 Loss: 1813.2649041811867\n",
      "[ -3.0760629    0.27549488  -2.50530274   1.31163763 -10.45830717\n",
      "   2.68181708   1.1867274    0.7151701    1.31163763]\n",
      "Iteration: 326 Loss: 1812.5762543467665\n",
      "[ -3.07545798   0.27544067  -2.50481007   1.31137969 -10.45625052\n",
      "   2.6812897    1.18649403   0.71502946   1.31137969]\n",
      "Iteration: 327 Loss: 1811.8878753357533\n",
      "[ -3.07485319   0.27538647  -2.50431749   1.3111218  -10.45419426\n",
      "   2.68076241   1.1862607    0.71488884   1.3111218 ]\n",
      "Iteration: 328 Loss: 1811.1997670416401\n",
      "[ -3.07424851   0.27533229  -2.50382501   1.31086397 -10.45213842\n",
      "   2.68023523   1.18602742   0.71474826   1.31086397]\n",
      "Iteration: 329 Loss: 1810.5119293579633\n",
      "[ -3.07364395   0.27527811  -2.50333262   1.31060618 -10.45008297\n",
      "   2.67970816   1.18579419   0.7146077    1.31060618]\n",
      "Iteration: 330 Loss: 1809.8243621783008\n",
      "[ -3.07303951   0.27522395  -2.50284033   1.31034845 -10.44802793\n",
      "   2.67918119   1.185561     0.71446717   1.31034845]\n",
      "Iteration: 331 Loss: 1809.1370653962722\n",
      "[ -3.07243518   0.27516979  -2.50234814   1.31009076 -10.4459733\n",
      "   2.67865432   1.18532785   0.71432667   1.31009076]\n",
      "Iteration: 332 Loss: 1808.4500389055388\n",
      "[ -3.07183098   0.27511565  -2.50185605   1.30983313 -10.44391907\n",
      "   2.67812755   1.18509475   0.7141862    1.30983313]\n",
      "Iteration: 333 Loss: 1807.7632825998044\n",
      "[ -3.0712269    0.27506152  -2.50136405   1.30957555 -10.44186524\n",
      "   2.67760089   1.1848617    0.71404575   1.30957555]\n",
      "Iteration: 334 Loss: 1807.0767963728133\n",
      "[ -3.07062293   0.27500739  -2.50087215   1.30931802 -10.43981182\n",
      "   2.67707433   1.18462869   0.71390533   1.30931802]\n",
      "Iteration: 335 Loss: 1806.3905801183523\n",
      "[ -3.07001908   0.27495328  -2.50038035   1.30906054 -10.4377588\n",
      "   2.67654788   1.18439573   0.71376494   1.30906054]\n",
      "Iteration: 336 Loss: 1805.7046337302515\n",
      "[ -3.06941536   0.27489918  -2.49988864   1.30880311 -10.43570618\n",
      "   2.67602153   1.18416282   0.71362458   1.30880311]\n",
      "Iteration: 337 Loss: 1805.0189571023798\n",
      "[ -3.06881175   0.27484509  -2.49939703   1.30854573 -10.43365397\n",
      "   2.67549528   1.18392995   0.71348424   1.30854573]\n",
      "Iteration: 338 Loss: 1804.33355012865\n",
      "[ -3.06820826   0.27479101  -2.49890552   1.3082884  -10.43160216\n",
      "   2.67496914   1.18369713   0.71334393   1.3082884 ]\n",
      "Iteration: 339 Loss: 1803.6484127030158\n",
      "[ -3.06760488   0.27473694  -2.4984141    1.30803112 -10.42955076\n",
      "   2.6744431    1.18346435   0.71320365   1.30803112]\n",
      "Iteration: 340 Loss: 1802.963544719473\n",
      "[ -3.06700163   0.27468289  -2.49792278   1.30777389 -10.42749976\n",
      "   2.67391716   1.18323162   0.7130634    1.30777389]\n",
      "Iteration: 341 Loss: 1802.2789460720583\n",
      "[ -3.0663985    0.27462884  -2.49743156   1.30751671 -10.42544916\n",
      "   2.67339133   1.18299893   0.71292317   1.30751671]\n",
      "Iteration: 342 Loss: 1801.5946166548513\n",
      "[ -3.06579548   0.2745748   -2.49694043   1.30725959 -10.42339897\n",
      "   2.6728656    1.18276629   0.71278297   1.30725959]\n",
      "Iteration: 343 Loss: 1800.910556361972\n",
      "[ -3.06519258   0.27452077  -2.4964494    1.30700251 -10.42134917\n",
      "   2.67233997   1.1825337    0.7126428    1.30700251]\n",
      "Iteration: 344 Loss: 1800.2267650875835\n",
      "[ -3.0645898    0.27446676  -2.49595846   1.30674548 -10.41929979\n",
      "   2.67181445   1.18230115   0.71250266   1.30674548]\n",
      "Iteration: 345 Loss: 1799.5432427258886\n",
      "[ -3.06398714   0.27441275  -2.49546763   1.30648851 -10.4172508\n",
      "   2.67128903   1.18206865   0.71236254   1.30648851]\n",
      "Iteration: 346 Loss: 1798.8599891711333\n",
      "[ -3.0633846    0.27435876  -2.49497689   1.30623158 -10.41520222\n",
      "   2.67076371   1.18183619   0.71222245   1.30623158]\n",
      "Iteration: 347 Loss: 1798.177004317604\n",
      "[ -3.06278218   0.27430478  -2.49448624   1.30597471 -10.41315404\n",
      "   2.67023849   1.18160378   0.71208239   1.30597471]\n",
      "Iteration: 348 Loss: 1797.494288059631\n",
      "[ -3.06217987   0.2742508   -2.49399569   1.30571789 -10.41110626\n",
      "   2.66971338   1.18137141   0.71194236   1.30571789]\n",
      "Iteration: 349 Loss: 1796.8118402915827\n",
      "[ -3.06157769   0.27419684  -2.49350524   1.30546111 -10.40905889\n",
      "   2.66918838   1.18113909   0.71180235   1.30546111]\n",
      "Iteration: 350 Loss: 1796.1296609078713\n",
      "[ -3.06097562   0.27414289  -2.49301489   1.30520439 -10.40701192\n",
      "   2.66866347   1.18090682   0.71166238   1.30520439]\n",
      "Iteration: 351 Loss: 1795.4477498029503\n",
      "[ -3.06037367   0.27408895  -2.49252463   1.30494772 -10.40496535\n",
      "   2.66813867   1.18067459   0.71152243   1.30494772]\n",
      "Iteration: 352 Loss: 1794.7661068713146\n",
      "[ -3.05977184   0.27403502  -2.49203447   1.3046911  -10.40291918\n",
      "   2.66761398   1.1804424    0.7113825    1.3046911 ]\n",
      "Iteration: 353 Loss: 1794.0847320075\n",
      "[ -3.05917012   0.2739811   -2.4915444    1.30443452 -10.40087342\n",
      "   2.66708938   1.18021027   0.71124261   1.30443452]\n",
      "Iteration: 354 Loss: 1793.4036251060843\n",
      "[ -3.05856853   0.27392719  -2.49105443   1.304178   -10.39882805\n",
      "   2.66656489   1.17997818   0.71110274   1.304178  ]\n",
      "Iteration: 355 Loss: 1792.7227860616872\n",
      "[ -3.05796705   0.27387329  -2.49056456   1.30392153 -10.39678309\n",
      "   2.6660405    1.17974613   0.7109629    1.30392153]\n",
      "Iteration: 356 Loss: 1792.042214768968\n",
      "[ -3.0573657    0.2738194   -2.49007478   1.30366511 -10.39473854\n",
      "   2.66551622   1.17951413   0.71082309   1.30366511]\n",
      "Iteration: 357 Loss: 1791.3619111226305\n",
      "[ -3.05676446   0.27376552  -2.4895851    1.30340874 -10.39269438\n",
      "   2.66499204   1.17928217   0.7106833    1.30340874]\n",
      "Iteration: 358 Loss: 1790.681875017417\n",
      "[ -3.05616333   0.27371165  -2.48909552   1.30315242 -10.39065063\n",
      "   2.66446796   1.17905026   0.71054354   1.30315242]\n",
      "Iteration: 359 Loss: 1790.002106348112\n",
      "[ -3.05556233   0.2736578   -2.48860603   1.30289616 -10.38860728\n",
      "   2.66394398   1.1788184    0.71040381   1.30289616]\n",
      "Iteration: 360 Loss: 1789.3226050095427\n",
      "[ -3.05496145   0.27360395  -2.48811664   1.30263994 -10.38656433\n",
      "   2.66342011   1.17858658   0.71026411   1.30263994]\n",
      "Iteration: 361 Loss: 1788.6433708965765\n",
      "[ -3.05436068   0.27355012  -2.48762734   1.30238377 -10.38452178\n",
      "   2.66289634   1.17835481   0.71012443   1.30238377]\n",
      "Iteration: 362 Loss: 1787.9644039041223\n",
      "[ -3.05376003   0.27349629  -2.48713814   1.30212765 -10.38247963\n",
      "   2.66237268   1.17812308   0.70998479   1.30212765]\n",
      "Iteration: 363 Loss: 1787.2857039271298\n",
      "[ -3.0531595    0.27344248  -2.48664904   1.30187158 -10.38043789\n",
      "   2.66184911   1.1778914    0.70984516   1.30187158]\n",
      "Iteration: 364 Loss: 1786.6072708605911\n",
      "[ -3.05255909   0.27338867  -2.48616003   1.30161557 -10.37839655\n",
      "   2.66132565   1.17765977   0.70970557   1.30161557]\n",
      "Iteration: 365 Loss: 1785.9291045995396\n",
      "[ -3.05195879   0.27333488  -2.48567112   1.3013596  -10.3763556\n",
      "   2.6608023    1.17742818   0.70956601   1.3013596 ]\n",
      "Iteration: 366 Loss: 1785.2512050390476\n",
      "[ -3.05135862   0.2732811   -2.48518231   1.30110369 -10.37431506\n",
      "   2.66027904   1.17719663   0.70942647   1.30110369]\n",
      "Iteration: 367 Loss: 1784.5735720742327\n",
      "[ -3.05075856   0.27322733  -2.48469359   1.30084782 -10.37227493\n",
      "   2.65975589   1.17696513   0.70928696   1.30084782]\n",
      "Iteration: 368 Loss: 1783.89620560025\n",
      "[ -3.05015862   0.27317356  -2.48420497   1.300592   -10.37023519\n",
      "   2.65923284   1.17673368   0.70914747   1.300592  ]\n",
      "Iteration: 369 Loss: 1783.2191055122985\n",
      "[ -3.04955879   0.27311981  -2.48371644   1.30033624 -10.36819585\n",
      "   2.6587099    1.17650227   0.70900802   1.30033624]\n",
      "Iteration: 370 Loss: 1782.542271705616\n",
      "[ -3.04895909   0.27306607  -2.48322801   1.30008052 -10.36615692\n",
      "   2.65818705   1.17627091   0.70886859   1.30008052]\n",
      "Iteration: 371 Loss: 1781.8657040754842\n",
      "[ -3.0483595    0.27301234  -2.48273968   1.29982486 -10.36411838\n",
      "   2.65766431   1.17603959   0.70872919   1.29982486]\n",
      "Iteration: 372 Loss: 1781.1894025172235\n",
      "[ -3.04776003   0.27295863  -2.48225144   1.29956925 -10.36208025\n",
      "   2.65714168   1.17580832   0.70858981   1.29956925]\n",
      "Iteration: 373 Loss: 1780.5133669261972\n",
      "[ -3.04716068   0.27290492  -2.4817633    1.29931368 -10.36004251\n",
      "   2.65661914   1.17557709   0.70845047   1.29931368]\n",
      "Iteration: 374 Loss: 1779.8375971978094\n",
      "[ -3.04656145   0.27285122  -2.48127525   1.29905817 -10.35800518\n",
      "   2.65609671   1.17534591   0.70831115   1.29905817]\n",
      "Iteration: 375 Loss: 1779.1620932275043\n",
      "[ -3.04596234   0.27279753  -2.4807873    1.2988027  -10.35596825\n",
      "   2.65557438   1.17511478   0.70817186   1.2988027 ]\n",
      "Iteration: 376 Loss: 1778.486854910768\n",
      "[ -3.04536334   0.27274385  -2.48029945   1.29854729 -10.35393172\n",
      "   2.65505215   1.17488369   0.70803259   1.29854729]\n",
      "Iteration: 377 Loss: 1777.8118821431276\n",
      "[ -3.04476446   0.27269019  -2.47981169   1.29829193 -10.35189559\n",
      "   2.65453003   1.17465264   0.70789336   1.29829193]\n",
      "Iteration: 378 Loss: 1777.1371748201516\n",
      "[ -3.0441657    0.27263653  -2.47932403   1.29803661 -10.34985986\n",
      "   2.65400801   1.17442165   0.70775415   1.29803661]\n",
      "Iteration: 379 Loss: 1776.4627328374497\n",
      "[ -3.04356705   0.27258289  -2.47883646   1.29778135 -10.34782453\n",
      "   2.65348609   1.17419069   0.70761497   1.29778135]\n",
      "Iteration: 380 Loss: 1775.788556090672\n",
      "[ -3.04296853   0.27252925  -2.47834899   1.29752614 -10.3457896\n",
      "   2.65296427   1.17395978   0.70747581   1.29752614]\n",
      "Iteration: 381 Loss: 1775.1146444755097\n",
      "[ -3.04237012   0.27247563  -2.47786162   1.29727098 -10.34375507\n",
      "   2.65244256   1.17372892   0.70733668   1.29727098]\n",
      "Iteration: 382 Loss: 1774.4409978876954\n",
      "[ -3.04177183   0.27242202  -2.47737434   1.29701587 -10.34172094\n",
      "   2.65192095   1.1734981    0.70719758   1.29701587]\n",
      "Iteration: 383 Loss: 1773.7676162230023\n",
      "[ -3.04117365   0.27236841  -2.47688716   1.2967608  -10.33968721\n",
      "   2.65139944   1.17326733   0.70705851   1.2967608 ]\n",
      "Iteration: 384 Loss: 1773.0944993772448\n",
      "[ -3.0405756    0.27231482  -2.47640007   1.29650579 -10.33765388\n",
      "   2.65087804   1.17303661   0.70691947   1.29650579]\n",
      "Iteration: 385 Loss: 1772.4216472462788\n",
      "[ -3.03997766   0.27226124  -2.47591308   1.29625083 -10.33562095\n",
      "   2.65035673   1.17280593   0.70678045   1.29625083]\n",
      "Iteration: 386 Loss: 1771.7490597260005\n",
      "[ -3.03937984   0.27220767  -2.47542618   1.29599592 -10.33358842\n",
      "   2.64983553   1.17257529   0.70664146   1.29599592]\n",
      "Iteration: 387 Loss: 1771.0767367123467\n",
      "[ -3.03878214   0.27215411  -2.47493938   1.29574106 -10.33155629\n",
      "   2.64931444   1.1723447    0.7065025    1.29574106]\n",
      "Iteration: 388 Loss: 1770.4046781012958\n",
      "[ -3.03818455   0.27210056  -2.47445268   1.29548625 -10.32952456\n",
      "   2.64879344   1.17211415   0.70636356   1.29548625]\n",
      "Iteration: 389 Loss: 1769.7328837888672\n",
      "[ -3.03758708   0.27204702  -2.47396607   1.29523148 -10.32749323\n",
      "   2.64827255   1.17188365   0.70622465   1.29523148]\n",
      "Iteration: 390 Loss: 1769.0613536711203\n",
      "[ -3.03698973   0.27199349  -2.47347956   1.29497677 -10.3254623\n",
      "   2.64775176   1.1716532    0.70608577   1.29497677]\n",
      "Iteration: 391 Loss: 1768.3900876441573\n",
      "[ -3.0363925    0.27193997  -2.47299314   1.29472211 -10.32343177\n",
      "   2.64723107   1.17142279   0.70594692   1.29472211]\n",
      "Iteration: 392 Loss: 1767.7190856041186\n",
      "[ -3.03579539   0.27188646  -2.47250682   1.2944675  -10.32140164\n",
      "   2.64671048   1.17119243   0.70580809   1.2944675 ]\n",
      "Iteration: 393 Loss: 1767.0483474471876\n",
      "[ -3.03519839   0.27183296  -2.47202059   1.29421294 -10.3193719\n",
      "   2.64619      1.17096211   0.70566929   1.29421294]\n",
      "Iteration: 394 Loss: 1766.3778730695872\n",
      "[ -3.03460151   0.27177948  -2.47153446   1.29395843 -10.31734257\n",
      "   2.64566962   1.17073184   0.70553052   1.29395843]\n",
      "Iteration: 395 Loss: 1765.707662367582\n",
      "[ -3.03400474   0.271726    -2.47104843   1.29370397 -10.31531363\n",
      "   2.64514934   1.17050161   0.70539177   1.29370397]\n",
      "Iteration: 396 Loss: 1765.0377152374772\n",
      "[ -3.0334081    0.27167253  -2.47056249   1.29344956 -10.3132851\n",
      "   2.64462917   1.17027143   0.70525306   1.29344956]\n",
      "Iteration: 397 Loss: 1764.3680315756176\n",
      "[ -3.03281157   0.27161908  -2.47007665   1.2931952  -10.31125696\n",
      "   2.64410909   1.17004129   0.70511437   1.2931952 ]\n",
      "Iteration: 398 Loss: 1763.6986112783907\n",
      "[ -3.03221516   0.27156563  -2.4695909    1.29294089 -10.30922922\n",
      "   2.64358912   1.1698112    0.7049757    1.29294089]\n",
      "Iteration: 399 Loss: 1763.029454242224\n",
      "[ -3.03161886   0.2715122   -2.46910525   1.29268663 -10.30720188\n",
      "   2.64306925   1.16958115   0.70483707   1.29268663]\n",
      "Iteration: 400 Loss: 1762.3605603635847\n",
      "[ -3.03102269   0.27145877  -2.46861969   1.29243242 -10.30517494\n",
      "   2.64254948   1.16935115   0.70469846   1.29243242]\n",
      "Iteration: 401 Loss: 1761.6919295389825\n",
      "[ -3.03042663   0.27140536  -2.46813423   1.29217826 -10.3031484\n",
      "   2.64202982   1.16912119   0.70455988   1.29217826]\n",
      "Iteration: 402 Loss: 1761.023561664966\n",
      "[ -3.02983069   0.27135196  -2.46764886   1.29192415 -10.30112225\n",
      "   2.64151026   1.16889128   0.70442133   1.29192415]\n",
      "Iteration: 403 Loss: 1760.3554566381258\n",
      "[ -3.02923486   0.27129856  -2.46716359   1.29167009 -10.29909651\n",
      "   2.64099079   1.16866142   0.7042828    1.29167009]\n",
      "Iteration: 404 Loss: 1759.6876143550926\n",
      "[ -3.02863915   0.27124518  -2.46667842   1.29141607 -10.29707116\n",
      "   2.64047144   1.16843159   0.7041443    1.29141607]\n",
      "Iteration: 405 Loss: 1759.0200347125383\n",
      "[ -3.02804356   0.27119181  -2.46619334   1.29116211 -10.29504621\n",
      "   2.63995218   1.16820182   0.70400583   1.29116211]\n",
      "Iteration: 406 Loss: 1758.3527176071746\n",
      "[ -3.02744809   0.27113845  -2.46570836   1.2909082  -10.29302166\n",
      "   2.63943303   1.16797209   0.70386738   1.2909082 ]\n",
      "Iteration: 407 Loss: 1757.6856629357544\n",
      "[ -3.02685273   0.2710851   -2.46522347   1.29065434 -10.29099751\n",
      "   2.63891397   1.1677424    0.70372897   1.29065434]\n",
      "Iteration: 408 Loss: 1757.0188705950707\n",
      "[ -3.02625749   0.27103176  -2.46473867   1.29040053 -10.28897376\n",
      "   2.63839502   1.16751276   0.70359058   1.29040053]\n",
      "Iteration: 409 Loss: 1756.3523404819575\n",
      "[ -3.02566237   0.27097843  -2.46425398   1.29014677 -10.2869504\n",
      "   2.63787618   1.16728317   0.70345221   1.29014677]\n",
      "Iteration: 410 Loss: 1755.6860724932894\n",
      "[ -3.02506737   0.27092511  -2.46376937   1.28989306 -10.28492744\n",
      "   2.63735743   1.16705362   0.70331388   1.28989306]\n",
      "Iteration: 411 Loss: 1755.020066525982\n",
      "[ -3.02447248   0.2708718   -2.46328487   1.2896394  -10.28290488\n",
      "   2.63683879   1.16682411   0.70317557   1.2896394 ]\n",
      "Iteration: 412 Loss: 1754.3543224769896\n",
      "[ -3.02387771   0.2708185   -2.46280045   1.28938579 -10.28088272\n",
      "   2.63632024   1.16659466   0.70303729   1.28938579]\n",
      "Iteration: 413 Loss: 1753.6888402433096\n",
      "[ -3.02328305   0.27076522  -2.46231614   1.28913223 -10.27886095\n",
      "   2.6358018    1.16636524   0.70289903   1.28913223]\n",
      "Iteration: 414 Loss: 1753.0236197219785\n",
      "[ -3.02268852   0.27071194  -2.46183192   1.28887871 -10.27683958\n",
      "   2.63528347   1.16613587   0.70276081   1.28887871]\n",
      "Iteration: 415 Loss: 1752.3586608100723\n",
      "[ -3.0220941    0.27065867  -2.46134779   1.28862525 -10.27481861\n",
      "   2.63476523   1.16590655   0.70262261   1.28862525]\n",
      "Iteration: 416 Loss: 1751.6939634047094\n",
      "[ -3.02149979   0.27060542  -2.46086376   1.28837184 -10.27279804\n",
      "   2.6342471    1.16567727   0.70248443   1.28837184]\n",
      "Iteration: 417 Loss: 1751.0295274030475\n",
      "[ -3.02090561   0.27055217  -2.46037982   1.28811848 -10.27077787\n",
      "   2.63372906   1.16544804   0.70234629   1.28811848]\n",
      "Iteration: 418 Loss: 1750.3653527022861\n",
      "[ -3.02031154   0.27049893  -2.45989598   1.28786517 -10.26875809\n",
      "   2.63321113   1.16521885   0.70220817   1.28786517]\n",
      "Iteration: 419 Loss: 1749.701439199663\n",
      "[ -3.01971758   0.27044571  -2.45941224   1.2876119  -10.26673871\n",
      "   2.6326933    1.1649897    0.70207008   1.2876119 ]\n",
      "Iteration: 420 Loss: 1749.0377867924578\n",
      "[ -3.01912375   0.2703925   -2.45892859   1.28735869 -10.26471972\n",
      "   2.63217558   1.1647606    0.70193201   1.28735869]\n",
      "Iteration: 421 Loss: 1748.3743953779908\n",
      "[ -3.01853003   0.27033929  -2.45844503   1.28710553 -10.26270114\n",
      "   2.63165795   1.16453155   0.70179398   1.28710553]\n",
      "Iteration: 422 Loss: 1747.711264853621\n",
      "[ -3.01793642   0.2702861   -2.45796157   1.28685242 -10.26068295\n",
      "   2.63114043   1.16430254   0.70165597   1.28685242]\n",
      "Iteration: 423 Loss: 1747.0483951167496\n",
      "[ -3.01734294   0.27023292  -2.4574782    1.28659935 -10.25866515\n",
      "   2.63062301   1.16407358   0.70151798   1.28659935]\n",
      "Iteration: 424 Loss: 1746.3857860648175\n",
      "[ -3.01674957   0.27017974  -2.45699493   1.28634634 -10.25664776\n",
      "   2.63010569   1.16384466   0.70138003   1.28634634]\n",
      "Iteration: 425 Loss: 1745.7234375953055\n",
      "[ -3.01615632   0.27012658  -2.45651176   1.28609337 -10.25463076\n",
      "   2.62958847   1.16361579   0.7012421    1.28609337]\n",
      "Iteration: 426 Loss: 1745.0613496057356\n",
      "[ -3.01556318   0.27007343  -2.45602868   1.28584046 -10.25261416\n",
      "   2.62907135   1.16338696   0.7011042    1.28584046]\n",
      "Iteration: 427 Loss: 1744.3995219936692\n",
      "[ -3.01497016   0.27002029  -2.45554569   1.2855876  -10.25059795\n",
      "   2.62855434   1.16315818   0.70096632   1.2855876 ]\n",
      "Iteration: 428 Loss: 1743.737954656708\n",
      "[ -3.01437726   0.26996716  -2.4550628    1.28533478 -10.24858214\n",
      "   2.62803743   1.16292944   0.70082848   1.28533478]\n",
      "Iteration: 429 Loss: 1743.0766474924956\n",
      "[ -3.01378447   0.26991404  -2.45458001   1.28508202 -10.24656673\n",
      "   2.62752062   1.16270074   0.70069066   1.28508202]\n",
      "Iteration: 430 Loss: 1742.4156003987134\n",
      "[ -3.0131918    0.26986093  -2.45409731   1.2848293  -10.24455171\n",
      "   2.62700391   1.1624721    0.70055286   1.2848293 ]\n",
      "Iteration: 431 Loss: 1741.7548132730838\n",
      "[ -3.01259925   0.26980783  -2.4536147    1.28457664 -10.24253709\n",
      "   2.6264873    1.16224349   0.7004151    1.28457664]\n",
      "Iteration: 432 Loss: 1741.0942860133714\n",
      "[ -3.01200681   0.26975474  -2.45313219   1.28432402 -10.24052286\n",
      "   2.62597079   1.16201493   0.70027736   1.28432402]\n",
      "Iteration: 433 Loss: 1740.4340185173785\n",
      "[ -3.01141449   0.26970166  -2.45264978   1.28407145 -10.23850904\n",
      "   2.62545439   1.16178642   0.70013965   1.28407145]\n",
      "Iteration: 434 Loss: 1739.774010682949\n",
      "[ -3.01082229   0.26964859  -2.45216746   1.28381894 -10.2364956\n",
      "   2.62493808   1.16155795   0.70000196   1.28381894]\n",
      "Iteration: 435 Loss: 1739.1142624079655\n",
      "[ -3.0102302    0.26959554  -2.45168523   1.28356647 -10.23448257\n",
      "   2.62442188   1.16132953   0.69986431   1.28356647]\n",
      "Iteration: 436 Loss: 1738.4547735903527\n",
      "[ -3.00963823   0.26954249  -2.4512031    1.28331405 -10.23246993\n",
      "   2.62390578   1.16110115   0.69972668   1.28331405]\n",
      "Iteration: 437 Loss: 1737.795544128075\n",
      "[ -3.00904638   0.26948945  -2.45072106   1.28306169 -10.23045768\n",
      "   2.62338978   1.16087281   0.69958907   1.28306169]\n",
      "Iteration: 438 Loss: 1737.1365739191351\n",
      "[ -3.00845464   0.26943643  -2.45023912   1.28280937 -10.22844583\n",
      "   2.62287389   1.16064453   0.6994515    1.28280937]\n",
      "Iteration: 439 Loss: 1736.4778628615782\n",
      "[ -3.00786302   0.26938341  -2.44975728   1.2825571  -10.22643438\n",
      "   2.62235809   1.16041628   0.69931395   1.2825571 ]\n",
      "Iteration: 440 Loss: 1735.8194108534876\n",
      "[ -3.00727152   0.2693304   -2.44927552   1.28230488 -10.22442332\n",
      "   2.6218424    1.16018808   0.69917643   1.28230488]\n",
      "Iteration: 441 Loss: 1735.161217792989\n",
      "[ -3.00668013   0.26927741  -2.44879387   1.28205271 -10.22241266\n",
      "   2.6213268    1.15995993   0.69903893   1.28205271]\n",
      "Iteration: 442 Loss: 1734.5032835782451\n",
      "[ -3.00608885   0.26922442  -2.4483123    1.28180059 -10.22040239\n",
      "   2.62081131   1.15973182   0.69890146   1.28180059]\n",
      "Iteration: 443 Loss: 1733.8456081074623\n",
      "[ -3.0054977    0.26917145  -2.44783084   1.28154852 -10.21839252\n",
      "   2.62029592   1.15950375   0.69876402   1.28154852]\n",
      "Iteration: 444 Loss: 1733.1881912788836\n",
      "[ -3.00490666   0.26911849  -2.44734946   1.2812965  -10.21638304\n",
      "   2.61978063   1.15927573   0.69862661   1.2812965 ]\n",
      "Iteration: 445 Loss: 1732.5310329907934\n",
      "[ -3.00431574   0.26906553  -2.44686819   1.28104453 -10.21437396\n",
      "   2.61926545   1.15904776   0.69848922   1.28104453]\n",
      "Iteration: 446 Loss: 1731.8741331415174\n",
      "[ -3.00372493   0.26901259  -2.446387     1.28079261 -10.21236528\n",
      "   2.61875036   1.15881983   0.69835186   1.28079261]\n",
      "Iteration: 447 Loss: 1731.217491629419\n",
      "[ -3.00313424   0.26895966  -2.44590591   1.28054074 -10.21035699\n",
      "   2.61823538   1.15859194   0.69821453   1.28054074]\n",
      "Iteration: 448 Loss: 1730.5611083529034\n",
      "[ -3.00254366   0.26890674  -2.44542492   1.28028892 -10.20834909\n",
      "   2.61772049   1.1583641    0.69807722   1.28028892]\n",
      "Iteration: 449 Loss: 1729.9049832104147\n",
      "[ -3.0019532    0.26885382  -2.44494402   1.28003715 -10.20634159\n",
      "   2.61720571   1.15813631   0.69793994   1.28003715]\n",
      "Iteration: 450 Loss: 1729.2491161004373\n",
      "[ -3.00136286   0.26880092  -2.44446321   1.27978542 -10.20433448\n",
      "   2.61669103   1.15790856   0.69780269   1.27978542]\n",
      "Iteration: 451 Loss: 1728.5935069214956\n",
      "[ -3.00077263   0.26874803  -2.4439825    1.27953375 -10.20232777\n",
      "   2.61617645   1.15768085   0.69766547   1.27953375]\n",
      "Iteration: 452 Loss: 1727.9381555721536\n",
      "[ -3.00018252   0.26869515  -2.44350189   1.27928213 -10.20032145\n",
      "   2.61566197   1.15745319   0.69752827   1.27928213]\n",
      "Iteration: 453 Loss: 1727.2830619510148\n",
      "[ -2.99959253   0.26864228  -2.44302136   1.27903055 -10.19831553\n",
      "   2.61514759   1.15722557   0.6973911    1.27903055]\n",
      "Iteration: 454 Loss: 1726.628225956724\n",
      "[ -2.99900265   0.26858942  -2.44254094   1.27877903 -10.19631\n",
      "   2.61463332   1.156998     0.69725395   1.27877903]\n",
      "Iteration: 455 Loss: 1725.973647487965\n",
      "[ -2.99841289   0.26853657  -2.4420606    1.27852755 -10.19430487\n",
      "   2.61411914   1.15677047   0.69711684   1.27852755]\n",
      "Iteration: 456 Loss: 1725.3193264434606\n",
      "[ -2.99782324   0.26848373  -2.44158037   1.27827612 -10.19230013\n",
      "   2.61360507   1.15654299   0.69697975   1.27827612]\n",
      "Iteration: 457 Loss: 1724.6652627219753\n",
      "[ -2.99723371   0.26843091  -2.44110022   1.27802475 -10.19029578\n",
      "   2.61309109   1.15631555   0.69684268   1.27802475]\n",
      "Iteration: 458 Loss: 1724.011456222312\n",
      "[ -2.9966443    0.26837809  -2.44062017   1.27777342 -10.18829183\n",
      "   2.61257722   1.15608816   0.69670565   1.27777342]\n",
      "Iteration: 459 Loss: 1723.3579068433135\n",
      "[ -2.996055     0.26832528  -2.44014022   1.27752214 -10.18628828\n",
      "   2.61206345   1.15586081   0.69656864   1.27752214]\n",
      "Iteration: 460 Loss: 1722.704614483863\n",
      "[ -2.99546581   0.26827248  -2.43966036   1.27727091 -10.18428511\n",
      "   2.61154978   1.15563351   0.69643166   1.27727091]\n",
      "Iteration: 461 Loss: 1722.051579042883\n",
      "[ -2.99487675   0.2682197   -2.43918059   1.27701974 -10.18228234\n",
      "   2.61103621   1.15540625   0.6962947    1.27701974]\n",
      "Iteration: 462 Loss: 1721.3988004193359\n",
      "[ -2.9942878    0.26816692  -2.43870092   1.27676861 -10.18027997\n",
      "   2.61052274   1.15517904   0.69615777   1.27676861]\n",
      "Iteration: 463 Loss: 1720.746278512224\n",
      "[ -2.99369896   0.26811415  -2.43822134   1.27651753 -10.17827799\n",
      "   2.61000938   1.15495187   0.69602087   1.27651753]\n",
      "Iteration: 464 Loss: 1720.0940132205887\n",
      "[ -2.99311024   0.2680614   -2.43774186   1.2762665  -10.1762764\n",
      "   2.60949611   1.15472474   0.695884     1.2762665 ]\n",
      "Iteration: 465 Loss: 1719.4420044435126\n",
      "[ -2.99252164   0.26800865  -2.43726247   1.27601551 -10.17427521\n",
      "   2.60898295   1.15449766   0.69574715   1.27601551]\n",
      "Iteration: 466 Loss: 1718.7902520801154\n",
      "[ -2.99193315   0.26795592  -2.43678317   1.27576458 -10.17227441\n",
      "   2.60846988   1.15427063   0.69561033   1.27576458]\n",
      "Iteration: 467 Loss: 1718.1387560295586\n",
      "[ -2.99134478   0.26790319  -2.43630397   1.2755137  -10.170274\n",
      "   2.60795692   1.15404364   0.69547353   1.2755137 ]\n",
      "Iteration: 468 Loss: 1717.4875161910434\n",
      "[ -2.99075652   0.26785048  -2.43582487   1.27526287 -10.16827398\n",
      "   2.60744406   1.15381669   0.69533677   1.27526287]\n",
      "Iteration: 469 Loss: 1716.8365324638098\n",
      "[ -2.99016838   0.26779777  -2.43534586   1.27501208 -10.16627436\n",
      "   2.6069313    1.15358979   0.69520003   1.27501208]\n",
      "Iteration: 470 Loss: 1716.185804747137\n",
      "[ -2.98958035   0.26774508  -2.43486694   1.27476135 -10.16427514\n",
      "   2.60641864   1.15336293   0.69506331   1.27476135]\n",
      "Iteration: 471 Loss: 1715.5353329403447\n",
      "[ -2.98899244   0.2676924   -2.43438811   1.27451066 -10.1622763\n",
      "   2.60590608   1.15313612   0.69492663   1.27451066]\n",
      "Iteration: 472 Loss: 1714.8851169427921\n",
      "[ -2.98840465   0.26763972  -2.43390939   1.27426002 -10.16027786\n",
      "   2.60539362   1.15290935   0.69478997   1.27426002]\n",
      "Iteration: 473 Loss: 1714.2351566538775\n",
      "[ -2.98781697   0.26758706  -2.43343075   1.27400944 -10.15827981\n",
      "   2.60488126   1.15268263   0.69465334   1.27400944]\n",
      "Iteration: 474 Loss: 1713.58545197304\n",
      "[ -2.98722941   0.26753441  -2.43295221   1.2737589  -10.15628216\n",
      "   2.604369     1.15245595   0.69451673   1.2737589 ]\n",
      "Iteration: 475 Loss: 1712.9360027997564\n",
      "[ -2.98664196   0.26748177  -2.43247376   1.27350841 -10.1542849\n",
      "   2.60385684   1.15222932   0.69438015   1.27350841]\n",
      "Iteration: 476 Loss: 1712.286809033544\n",
      "[ -2.98605463   0.26742914  -2.43199541   1.27325797 -10.15228803\n",
      "   2.60334479   1.15200273   0.6942436    1.27325797]\n",
      "Iteration: 477 Loss: 1711.6378705739596\n",
      "[ -2.98546741   0.26737652  -2.43151715   1.27300758 -10.15029155\n",
      "   2.60283283   1.15177619   0.69410707   1.27300758]\n",
      "Iteration: 478 Loss: 1710.9891873206\n",
      "[ -2.98488031   0.2673239   -2.43103898   1.27275724 -10.14829547\n",
      "   2.60232098   1.15154968   0.69397058   1.27275724]\n",
      "Iteration: 479 Loss: 1710.3407591731004\n",
      "[ -2.98429333   0.2672713   -2.43056091   1.27250695 -10.14629977\n",
      "   2.60180923   1.15132323   0.69383411   1.27250695]\n",
      "Iteration: 480 Loss: 1709.6925860311362\n",
      "[ -2.98370646   0.26721871  -2.43008294   1.27225671 -10.14430448\n",
      "   2.60129757   1.15109682   0.69369766   1.27225671]\n",
      "Iteration: 481 Loss: 1709.0446677944224\n",
      "[ -2.9831197    0.26716613  -2.42960505   1.27200652 -10.14230957\n",
      "   2.60078602   1.15087045   0.69356124   1.27200652]\n",
      "Iteration: 482 Loss: 1708.3970043627126\n",
      "[ -2.98253306   0.26711356  -2.42912727   1.27175637 -10.14031505\n",
      "   2.60027457   1.15064413   0.69342485   1.27175637]\n",
      "Iteration: 483 Loss: 1707.7495956358002\n",
      "[ -2.98194654   0.26706101  -2.42864957   1.27150628 -10.13832093\n",
      "   2.59976322   1.15041785   0.69328849   1.27150628]\n",
      "Iteration: 484 Loss: 1707.1024415135191\n",
      "[ -2.98136013   0.26700846  -2.42817197   1.27125623 -10.1363272\n",
      "   2.59925196   1.15019162   0.69315215   1.27125623]\n",
      "Iteration: 485 Loss: 1706.455541895741\n",
      "[ -2.98077384   0.26695592  -2.42769446   1.27100624 -10.13433386\n",
      "   2.59874081   1.14996543   0.69301584   1.27100624]\n",
      "Iteration: 486 Loss: 1705.8088966823775\n",
      "[ -2.98018766   0.26690339  -2.42721705   1.27075629 -10.13234092\n",
      "   2.59822976   1.14973929   0.69287956   1.27075629]\n",
      "Iteration: 487 Loss: 1705.16250577338\n",
      "[ -2.9796016    0.26685087  -2.42673973   1.27050639 -10.13034836\n",
      "   2.59771882   1.14951319   0.6927433    1.27050639]\n",
      "Iteration: 488 Loss: 1704.5163690687386\n",
      "[ -2.97901565   0.26679837  -2.42626251   1.27025654 -10.1283562\n",
      "   2.59720797   1.14928713   0.69260707   1.27025654]\n",
      "Iteration: 489 Loss: 1703.8704864684837\n",
      "[ -2.97842982   0.26674587  -2.42578537   1.27000674 -10.12636443\n",
      "   2.59669722   1.14906112   0.69247087   1.27000674]\n",
      "Iteration: 490 Loss: 1703.2248578726833\n",
      "[ -2.9778441    0.26669338  -2.42530834   1.26975699 -10.12437305\n",
      "   2.59618657   1.14883516   0.69233469   1.26975699]\n",
      "Iteration: 491 Loss: 1702.5794831814464\n",
      "[ -2.9772585    0.26664091  -2.42483139   1.26950729 -10.12238207\n",
      "   2.59567602   1.14860923   0.69219854   1.26950729]\n",
      "Iteration: 492 Loss: 1701.9343622949216\n",
      "[ -2.97667302   0.26658844  -2.42435454   1.26925764 -10.12039147\n",
      "   2.59516558   1.14838336   0.69206242   1.26925764]\n",
      "Iteration: 493 Loss: 1701.2894951132944\n",
      "[ -2.97608764   0.26653598  -2.42387779   1.26900804 -10.11840127\n",
      "   2.59465523   1.14815752   0.69192632   1.26900804]\n",
      "Iteration: 494 Loss: 1700.6448815367914\n",
      "[ -2.97550239   0.26648354  -2.42340112   1.26875848 -10.11641145\n",
      "   2.59414498   1.14793173   0.69179025   1.26875848]\n",
      "Iteration: 495 Loss: 1700.0005214656783\n",
      "[ -2.97491725   0.2664311   -2.42292455   1.26850898 -10.11442203\n",
      "   2.59363484   1.14770599   0.69165421   1.26850898]\n",
      "Iteration: 496 Loss: 1699.356414800259\n",
      "[ -2.97433222   0.26637868  -2.42244808   1.26825952 -10.112433\n",
      "   2.59312479   1.14748029   0.69151819   1.26825952]\n",
      "Iteration: 497 Loss: 1698.712561440878\n",
      "[ -2.97374731   0.26632626  -2.4219717    1.26801011 -10.11044436\n",
      "   2.59261484   1.14725464   0.69138221   1.26801011]\n",
      "Iteration: 498 Loss: 1698.0689612879185\n",
      "[ -2.97316251   0.26627386  -2.42149541   1.26776076 -10.10845611\n",
      "   2.592105     1.14702902   0.69124624   1.26776076]\n",
      "Iteration: 499 Loss: 1697.4256142418026\n",
      "[ -2.97257783   0.26622147  -2.42101922   1.26751145 -10.10646826\n",
      "   2.59159525   1.14680346   0.69111031   1.26751145]\n",
      "Iteration: 500 Loss: 1696.7825202029908\n",
      "[ -2.97199327   0.26616908  -2.42054312   1.26726219 -10.10448079\n",
      "   2.59108561   1.14657794   0.6909744    1.26726219]\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_weighted_linear(X_train, y_train, x_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцінимо, наскільки модель помиляється на тестовій вибірці:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.array([predict_weighted_linear(X_test, y_test, X_test[i]) for i in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame(y_test - y_test_pred, columns=[\"residual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5,1,'Error Distribution')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcbc694828>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcbc6945f8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df_residuals.plot.bar(figsize=(10, 5))\n",
    "ax.legend().remove()\n",
    "ax.get_xaxis().set(ticklabels=[])\n",
    "ax.set(xlabel=\"Example #\")\n",
    "ax.set(ylabel=\"Prediction Error, $\")\n",
    "ax.set(title=\"Residuals\")\n",
    "\n",
    "bins = np.arange(-3000, 3001, 100)\n",
    "ticks = np.arange(-3000, 3001, 500)\n",
    "\n",
    "ax = df_residuals.hist(bins=bins, figsize=(10, 5))[0][0]\n",
    "ax.set(xticks=ticks)\n",
    "ax.set(xlabel=\"Prediction Error, $\")\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(title=\"Error Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестування"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустіть комірку нижче, щоб перевірити правильність вашого коду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.37333333 -33.74333333  -9.0359     -25.68333333] [-37.12, -101.23, -27.108, -77.05]\n",
      "[-2.63733333 -2.67433333 -5.10359    -3.96833333] [-7.912, -8.023, -15.311, -11.905]\n"
     ]
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "salmon",
       "message": "..FF........\n======================================================================\nFAIL: test_cost_gradient_should_compute_correct_gradient_unweighted (__main__.WLRTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 64, in test_cost_gradient_should_compute_correct_gradient_unweighted\n  File \"Cell Tests\", line 21, in assertArrayEquals\nAssertionError: False is not true\n\n======================================================================\nFAIL: test_cost_gradient_should_compute_correct_gradient_weighted (__main__.WLRTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 71, in test_cost_gradient_should_compute_correct_gradient_weighted\n  File \"Cell Tests\", line 21, in assertArrayEquals\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 12 tests in 0.006s\n\nFAILED (failures=2)\n",
       "previous": 0
      },
      "text/plain": [
       "Fail"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..FF........\n",
      "======================================================================\n",
      "FAIL: test_cost_gradient_should_compute_correct_gradient_unweighted (__main__.WLRTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 64, in test_cost_gradient_should_compute_correct_gradient_unweighted\n",
      "  File \"Cell Tests\", line 21, in assertArrayEquals\n",
      "AssertionError: False is not true\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_cost_gradient_should_compute_correct_gradient_weighted (__main__.WLRTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 71, in test_cost_gradient_should_compute_correct_gradient_weighted\n",
      "  File \"Cell Tests\", line 21, in assertArrayEquals\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.006s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=2>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "\n",
    "class WLRTests(unittest.TestCase):\n",
    "\n",
    "    x = np.array([1, -0.5, 3, 1])\n",
    "    X = np.array([\n",
    "        [1, -0.5, 3, 1],\n",
    "        [2, 8, -0.33, 5],\n",
    "        [0, 0, 0, 0]\n",
    "    ])\n",
    "    y = np.array([40, 100, 12])\n",
    "    theta = np.array([2, 5, 7, 9])\n",
    "    eps = 0.001\n",
    "\n",
    "    def assertFloatEquals(self, a, b):\n",
    "        self.assertTrue(np.abs(a - b) < self.eps)\n",
    "    \n",
    "    def assertArrayEquals(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        self.assertEqual(a.shape, b.shape)\n",
    "        self.assertTrue(np.all(np.abs(a - b) < self.eps))\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_prediction_for_1_example(self):\n",
    "        expected = 29.5\n",
    "        actual = predict_linear(self.theta, self.x)\n",
    "        self.assertEqual(actual, expected)\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_predictions_for_multiple_examples(self):\n",
    "        expected = [29.5, 86.69, 0]\n",
    "        actual = (predict_linear(self.theta, self.X))\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_get_example_weights_should_return_properly_shaped_vector(self):\n",
    "        weights = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertTrue(weights.shape[0] == self.X.shape[0])\n",
    "    \n",
    "    def test_get_example_weights_should_compute_correct_weights(self):\n",
    "        expected = [1.000, 0.134, 0.798]\n",
    "        actual = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_function_should_compute_correct_cost_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = 71.901\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_function_should_compute_correct_cost_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = 18.860\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "\n",
    "    def test_cost_gradient_should_return_properly_shaped_vector(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        grad = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertTrue(grad.shape == self.theta.shape)\n",
    "        \n",
    "    def test_cost_gradient_should_compute_correct_gradient_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = [-37.12, -101.23, -27.108, -77.05]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        print(actual, expected)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_gradient_should_compute_correct_gradient_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = [-7.912, -8.023, -15.311, -11.905]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        print(actual, expected)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_update_model_weights_should_not_update_when_gradient_is_zero(self):\n",
    "        grad = np.zeros(self.theta.shape[0])\n",
    "        theta_new = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(theta_new, self.theta)\n",
    "    \n",
    "    def test_update_model_weights_should_update_with_complete_gradient_if_learning_rate_is_one(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [0.65, 5.89, 6.84, 8.02]\n",
    "        actual = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_update_model_weights_should_take_learning_rate_into_account(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [1.730, 5.178, 6.968, 8.804]\n",
    "        actual = update_model_weights(self.theta, learning_rate=0.2, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
